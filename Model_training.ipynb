{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning HW2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "WQloVwIOtTSM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98353edc-dab1-4319-d2d3-12307b383dfc"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import BatchNormalization, add, merge, Concatenate, Dropout, Conv1D, MaxPooling1D, LSTM, Embedding, Dense, TimeDistributed, Bidirectional, Flatten, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l2\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "C3XvsrD3k7wl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "0f5b792a-6223-40e4-c402-6c3586a698db"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd \n",
        "df=pd.read_csv('gdrive/My Drive/train.csv')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iNsAGxNaiRVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The custom accuracy metric used for this task\n",
        "def accuracy(y_true, y_pred):\n",
        "    y = tf.argmax(y_true, axis =- 1)\n",
        "    y_ = tf.argmax(y_pred, axis =- 1)\n",
        "    mask = tf.greater(y, 0)\n",
        "    return K.cast(K.equal(tf.boolean_mask(y, mask), tf.boolean_mask(y_, mask)), K.floatx())\n",
        "\n",
        "# Maps the sequence to a one-hot encoding\n",
        "def onehot_to_seq(oh_seq, index):\n",
        "    s = ''           \n",
        "    for o in oh_seq:\n",
        "        i = np.argmax(o)\n",
        "        if i != 0:\n",
        "            s += index[i]\n",
        "        else:\n",
        "            break\n",
        "    return s\n",
        "\n",
        "# prints the results\n",
        "def print_results(x, y_, revsere_decoder_index):\n",
        "    # print(\"input     : \" + str(x))\n",
        "    # print(\"prediction: \" + str(onehot_to_seq(y_, revsere_decoder_index).upper()))\n",
        "    print(str(onehot_to_seq(y_, revsere_decoder_index).upper()))\n",
        "\n",
        "# Computes and returns the n-grams of a particualr sequence, defaults to trigrams\n",
        "def seq2ngrams(seqs, n = 3):\n",
        "    return np.array([[seq[i : i + n] for i in range(len(seq))] for seq in seqs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kT1HquVBfjQP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('gdrive/My Drive/train.csv')\n",
        "test_df = pd.read_csv('gdrive/My Drive/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GE883vwriyuF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "maxlen_seq = 700\n",
        "\n",
        "# Loading and converting the inputs to trigrams\n",
        "train_input_seqs, train_target_seqs = train_df[['input', 'expected']][(train_df.len <= maxlen_seq)].values.T\n",
        "train_input_grams = seq2ngrams(train_input_seqs,1) #change from 3 to 2\n",
        "\n",
        "# Same for test\n",
        "test_input_seqs = test_df['input'].values.T\n",
        "test_input_grams = seq2ngrams(test_input_seqs,1) #change from 3 to 2\n",
        "\n",
        "# Initializing and defining the tokenizer encoders and decoders based on the train set\n",
        "tokenizer_encoder = Tokenizer()\n",
        "tokenizer_encoder.fit_on_texts(train_input_grams)\n",
        "tokenizer_decoder = Tokenizer(char_level = True)\n",
        "tokenizer_decoder.fit_on_texts(train_target_seqs)\n",
        "\n",
        "# Using the tokenizer to encode and decode the sequences for use in training\n",
        "# Inputs\n",
        "train_input_data = tokenizer_encoder.texts_to_sequences(train_input_grams)\n",
        "train_input_data = sequence.pad_sequences(train_input_data, maxlen = maxlen_seq, padding = 'post')\n",
        "\n",
        "# Targets\n",
        "train_target_data = tokenizer_decoder.texts_to_sequences(train_target_seqs)\n",
        "train_target_data = sequence.pad_sequences(train_target_data, maxlen = maxlen_seq, padding = 'post')\n",
        "train_target_data = to_categorical(train_target_data)\n",
        "\n",
        "# Use the same tokenizer defined on train for tokenization of test\n",
        "test_input_data = tokenizer_encoder.texts_to_sequences(test_input_grams)\n",
        "test_input_data = sequence.pad_sequences(test_input_data, maxlen = maxlen_seq, padding = 'post')\n",
        "\n",
        "# Computing the number of words and number of tags to be passed as parameters to the keras model\n",
        "n_words = len(tokenizer_encoder.word_index) + 1\n",
        "n_tags = len(tokenizer_decoder.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bUEm4ctXC_x5"
      },
      "cell_type": "markdown",
      "source": [
        "# Model "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ykR5OBaxCXtH",
        "outputId": "ccddc721-6244-4dce-d460-211b213f336e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1462
        }
      },
      "cell_type": "code",
      "source": [
        "input = Input(shape = (maxlen_seq,)) \n",
        "\n",
        "x = Embedding(input_dim = n_words, output_dim = 128, input_length = maxlen_seq)(input)\n",
        "\n",
        "#dropout the outpout\n",
        "x=Dropout(0.1)(x)\n",
        "\n",
        "#from the embeding\n",
        "d = Bidirectional(LSTM(units = 128, return_sequences = True, recurrent_dropout = 0.1))(x)\n",
        "\n",
        "#from the simplest model\n",
        "A = Conv1D(64, kernel_size=11, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "A = MaxPooling1D(pool_size=2, strides=1, padding='same')(A)\n",
        "A= BatchNormalization()(A)\n",
        "\n",
        "#dropout the outpout\n",
        "A= Dropout(0.1)(A)\n",
        "\n",
        "B = Conv1D(64, kernel_size=7, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.001))(A)\n",
        "B = MaxPooling1D(pool_size=2, strides=1, padding='same')(B)\n",
        "B= BatchNormalization()(B)\n",
        "\n",
        "#dropout the outpout\n",
        "B= Dropout(0.1)(B)\n",
        "\n",
        "C = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.001))(B)\n",
        "C = MaxPooling1D(pool_size=2, strides=1, padding='same')(C)\n",
        "C= BatchNormalization()(C)\n",
        "\n",
        "#dropout the outpout\n",
        "C=Dropout(0.1)(C)\n",
        "\n",
        "f=Bidirectional(LSTM(units = 128, return_sequences = True, recurrent_dropout = 0.1))(C)\n",
        "\n",
        "\n",
        "#from the complex model\n",
        "a = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "a = MaxPooling1D(pool_size=2, strides=1, padding='same')(a)\n",
        "a= BatchNormalization()(a)\n",
        "\n",
        "b = Conv1D(64, kernel_size=7, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "b = MaxPooling1D(pool_size=2, strides=1, padding='same')(b)\n",
        "b= BatchNormalization()(b)\n",
        "\n",
        "c = Conv1D(64, kernel_size=11, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "c = MaxPooling1D(pool_size=2, strides=1, padding='same')(c)\n",
        "c= BatchNormalization()(c)\n",
        "\n",
        "\n",
        "e = add([a,b,c])\n",
        "\n",
        "e= Dropout(0.1)(e)\n",
        "e = Bidirectional(LSTM(units = 128, return_sequences = True, recurrent_dropout = 0.1))(e)\n",
        "\n",
        "\n",
        "#dropout the outpouts\n",
        "d= Dropout(0.1)(d)\n",
        "\n",
        "e= Dropout(0.1)(e)\n",
        "\n",
        "f= Dropout(0.1)(f)\n",
        "\n",
        "g = add([d,e,f])\n",
        "h = Bidirectional(LSTM(units = 128, return_sequences = True, recurrent_dropout = 0.1))(g)\n",
        "\n",
        "h= Dropout (0.25)(h)\n",
        "i = TimeDistributed(Dense((256), activation='relu', kernel_regularizer=l2(0.001)))(h)\n",
        "\n",
        "output = TimeDistributed(Dense(n_tags, activation = \"softmax\"))(i)\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 700)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 700, 128)     2816        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 700, 128)     0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 700, 64)      90176       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 700, 64)      0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 700, 64)      256         max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 700, 64)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 700, 64)      28736       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 700, 64)      0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 700, 64)      256         max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 700, 64)      24640       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 700, 64)      57408       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 700, 64)      90176       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 700, 64)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 700, 64)      0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 700, 64)      0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 700, 64)      0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 700, 64)      12352       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 700, 64)      256         max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 700, 64)      256         max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 700, 64)      256         max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 700, 64)      0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 700, 64)      0           batch_normalization_4[0][0]      \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "                                                                 batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 700, 64)      256         max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 700, 64)      0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 700, 64)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 700, 256)     263168      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 700, 256)     197632      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 700, 256)     197632      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 700, 256)     0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 700, 256)     0           bidirectional_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 700, 256)     0           bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 700, 256)     0           dropout_6[0][0]                  \n",
            "                                                                 dropout_7[0][0]                  \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 700, 256)     394240      add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 700, 256)     0           bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 700, 256)     65792       dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 700, 9)       2313        time_distributed_1[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 1,428,617\n",
            "Trainable params: 1,427,849\n",
            "Non-trainable params: 768\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sG5GhfgQEG4V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setting up the model with categorical x-entropy loss and the custom accuracy function as accuracy\n",
        "model.compile(optimizer = \"nadam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\", accuracy])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4McuwyE8HeZa"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "h0VNc_ijMNee",
        "outputId": "218c392c-94c0-4e8f-afc3-e226ca57ef80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3471
        }
      },
      "cell_type": "code",
      "source": [
        "filepath=\"weights_model3_128_LSTM_1gram.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "early_stopping_monitor = EarlyStopping(patience=10)\n",
        "\n",
        "# Splitting the data for train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_input_data, train_target_data, test_size = .1, random_state = 0)\n",
        "\n",
        "# Training the model on the training data and validating using the validation set\n",
        "model.fit(X_train,  y_train, batch_size = 210, epochs = 50, validation_data = (X_val, y_val), callbacks=callbacks_list, verbose = 1) \n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5072 samples, validate on 564 samples\n",
            "Epoch 1/50\n",
            "5072/5072 [==============================] - 542s 107ms/step - loss: 0.4637 - acc: 0.8442 - accuracy: 0.6351 - val_loss: 0.6267 - val_acc: 0.7954 - val_accuracy: 0.5224\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.52239, saving model to weights_model3_128_LSTM_1gram.best.hdf5\n",
            "Epoch 2/50\n",
            "5072/5072 [==============================] - 516s 102ms/step - loss: 0.4612 - acc: 0.8447 - accuracy: 0.6361 - val_loss: 0.5134 - val_acc: 0.8273 - val_accuracy: 0.5970\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.52239 to 0.59697, saving model to weights_model3_128_LSTM_1gram.best.hdf5\n",
            "Epoch 3/50\n",
            "5072/5072 [==============================] - 517s 102ms/step - loss: 0.4536 - acc: 0.8467 - accuracy: 0.6409 - val_loss: 0.5163 - val_acc: 0.8268 - val_accuracy: 0.5958\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.59697\n",
            "Epoch 4/50\n",
            "5072/5072 [==============================] - 531s 105ms/step - loss: 0.4494 - acc: 0.8482 - accuracy: 0.6444 - val_loss: 0.5593 - val_acc: 0.8126 - val_accuracy: 0.5628\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.59697\n",
            "Epoch 5/50\n",
            "5072/5072 [==============================] - 532s 105ms/step - loss: 0.4690 - acc: 0.8422 - accuracy: 0.6303 - val_loss: 0.5353 - val_acc: 0.8241 - val_accuracy: 0.5895\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.59697\n",
            "Epoch 6/50\n",
            "5072/5072 [==============================] - 530s 105ms/step - loss: 0.4504 - acc: 0.8487 - accuracy: 0.6456 - val_loss: 0.5516 - val_acc: 0.8196 - val_accuracy: 0.5790\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.59697\n",
            "Epoch 7/50\n",
            "5072/5072 [==============================] - 538s 106ms/step - loss: 0.4469 - acc: 0.8491 - accuracy: 0.6465 - val_loss: 0.5541 - val_acc: 0.8163 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.59697\n",
            "Epoch 8/50\n",
            "5072/5072 [==============================] - 539s 106ms/step - loss: 0.4448 - acc: 0.8495 - accuracy: 0.6474 - val_loss: 0.5726 - val_acc: 0.8040 - val_accuracy: 0.5425\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.59697\n",
            "Epoch 9/50\n",
            "5072/5072 [==============================] - 529s 104ms/step - loss: 0.4429 - acc: 0.8503 - accuracy: 0.6494 - val_loss: 0.5409 - val_acc: 0.8188 - val_accuracy: 0.5770\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.59697\n",
            "Epoch 10/50\n",
            "5072/5072 [==============================] - 520s 102ms/step - loss: 0.4413 - acc: 0.8503 - accuracy: 0.6492 - val_loss: 0.6616 - val_acc: 0.7918 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.59697\n",
            "Epoch 11/50\n",
            "5072/5072 [==============================] - 512s 101ms/step - loss: 0.4619 - acc: 0.8466 - accuracy: 0.6408 - val_loss: 0.5376 - val_acc: 0.8213 - val_accuracy: 0.5831\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.59697\n",
            "Epoch 12/50\n",
            "5072/5072 [==============================] - 509s 100ms/step - loss: 0.4407 - acc: 0.8516 - accuracy: 0.6523 - val_loss: 0.5214 - val_acc: 0.8275 - val_accuracy: 0.5974\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.59697 to 0.59743, saving model to weights_model3_128_LSTM_1gram.best.hdf5\n",
            "Epoch 13/50\n",
            "5072/5072 [==============================] - 504s 99ms/step - loss: 0.4315 - acc: 0.8535 - accuracy: 0.6566 - val_loss: 0.5308 - val_acc: 0.8248 - val_accuracy: 0.5910\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.59743\n",
            "Epoch 14/50\n",
            "5072/5072 [==============================] - 500s 99ms/step - loss: 0.4286 - acc: 0.8542 - accuracy: 0.6585 - val_loss: 0.5210 - val_acc: 0.8267 - val_accuracy: 0.5954\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.59743\n",
            "Epoch 15/50\n",
            "5072/5072 [==============================] - 497s 98ms/step - loss: 0.4512 - acc: 0.8470 - accuracy: 0.6417 - val_loss: 0.5349 - val_acc: 0.8266 - val_accuracy: 0.5952\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.59743\n",
            "Epoch 16/50\n",
            "5072/5072 [==============================] - 494s 97ms/step - loss: 0.4300 - acc: 0.8544 - accuracy: 0.6590 - val_loss: 0.5270 - val_acc: 0.8247 - val_accuracy: 0.5909\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.59743\n",
            "Epoch 17/50\n",
            "5072/5072 [==============================] - 501s 99ms/step - loss: 0.5899 - acc: 0.8089 - accuracy: 0.5535 - val_loss: 0.5866 - val_acc: 0.8110 - val_accuracy: 0.5594\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.59743\n",
            "Epoch 18/50\n",
            "5072/5072 [==============================] - 524s 103ms/step - loss: 0.5113 - acc: 0.8358 - accuracy: 0.6151 - val_loss: 0.5477 - val_acc: 0.8232 - val_accuracy: 0.5874\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.59743\n",
            "Epoch 19/50\n",
            "5072/5072 [==============================] - 532s 105ms/step - loss: 0.4721 - acc: 0.8457 - accuracy: 0.6385 - val_loss: 0.5392 - val_acc: 0.8237 - val_accuracy: 0.5886\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.59743\n",
            "Epoch 20/50\n",
            "5072/5072 [==============================] - 532s 105ms/step - loss: 0.4526 - acc: 0.8498 - accuracy: 0.6483 - val_loss: 0.5432 - val_acc: 0.8231 - val_accuracy: 0.5871\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.59743\n",
            "Epoch 21/50\n",
            "5072/5072 [==============================] - 531s 105ms/step - loss: 0.4434 - acc: 0.8515 - accuracy: 0.6520 - val_loss: 0.5691 - val_acc: 0.8139 - val_accuracy: 0.5657\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.59743\n",
            "Epoch 22/50\n",
            "5072/5072 [==============================] - 529s 104ms/step - loss: 0.4426 - acc: 0.8508 - accuracy: 0.6506 - val_loss: 0.5418 - val_acc: 0.8214 - val_accuracy: 0.5830\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.59743\n",
            "Epoch 23/50\n",
            "5072/5072 [==============================] - 530s 104ms/step - loss: 0.4309 - acc: 0.8541 - accuracy: 0.6584 - val_loss: 0.5787 - val_acc: 0.8043 - val_accuracy: 0.5432\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.59743\n",
            "Epoch 24/50\n",
            "5072/5072 [==============================] - 526s 104ms/step - loss: 0.4294 - acc: 0.8546 - accuracy: 0.6595 - val_loss: 0.5590 - val_acc: 0.8150 - val_accuracy: 0.5683\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.59743\n",
            "Epoch 25/50\n",
            "5072/5072 [==============================] - 512s 101ms/step - loss: 0.4289 - acc: 0.8544 - accuracy: 0.6588 - val_loss: 0.5714 - val_acc: 0.8104 - val_accuracy: 0.5575\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.59743\n",
            "Epoch 26/50\n",
            "5072/5072 [==============================] - 510s 101ms/step - loss: 0.4195 - acc: 0.8575 - accuracy: 0.6661 - val_loss: 0.5523 - val_acc: 0.8187 - val_accuracy: 0.5767\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.59743\n",
            "Epoch 27/50\n",
            "5072/5072 [==============================] - 508s 100ms/step - loss: 0.4160 - acc: 0.8582 - accuracy: 0.6679 - val_loss: 0.5362 - val_acc: 0.8214 - val_accuracy: 0.5833\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.59743\n",
            "Epoch 28/50\n",
            "5072/5072 [==============================] - 502s 99ms/step - loss: 0.4150 - acc: 0.8582 - accuracy: 0.6678 - val_loss: 0.5542 - val_acc: 0.8154 - val_accuracy: 0.5692\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.59743\n",
            "Epoch 29/50\n",
            "5072/5072 [==============================] - 499s 98ms/step - loss: 0.4152 - acc: 0.8580 - accuracy: 0.6673 - val_loss: 0.6321 - val_acc: 0.7948 - val_accuracy: 0.5209\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.59743\n",
            "Epoch 30/50\n",
            "5072/5072 [==============================] - 496s 98ms/step - loss: 0.4190 - acc: 0.8570 - accuracy: 0.6649 - val_loss: 0.6842 - val_acc: 0.7876 - val_accuracy: 0.5042\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.59743\n",
            "Epoch 31/50\n",
            "5072/5072 [==============================] - 503s 99ms/step - loss: 0.4337 - acc: 0.8533 - accuracy: 0.6565 - val_loss: 0.5398 - val_acc: 0.8231 - val_accuracy: 0.5871\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.59743\n",
            "Epoch 32/50\n",
            "5072/5072 [==============================] - 536s 106ms/step - loss: 0.4123 - acc: 0.8599 - accuracy: 0.6717 - val_loss: 0.6953 - val_acc: 0.7907 - val_accuracy: 0.5116\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.59743\n",
            "Epoch 33/50\n",
            "5072/5072 [==============================] - 542s 107ms/step - loss: 0.4116 - acc: 0.8595 - accuracy: 0.6709 - val_loss: 0.5887 - val_acc: 0.8081 - val_accuracy: 0.5520\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.59743\n",
            "Epoch 34/50\n",
            "5072/5072 [==============================] - 545s 107ms/step - loss: 0.4316 - acc: 0.8555 - accuracy: 0.6615 - val_loss: 0.5663 - val_acc: 0.8209 - val_accuracy: 0.5820\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.59743\n",
            "Epoch 35/50\n",
            "5072/5072 [==============================] - 540s 106ms/step - loss: 0.4129 - acc: 0.8610 - accuracy: 0.6745 - val_loss: 0.6036 - val_acc: 0.8097 - val_accuracy: 0.5558\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.59743\n",
            "Epoch 36/50\n",
            "5072/5072 [==============================] - 543s 107ms/step - loss: 0.4077 - acc: 0.8613 - accuracy: 0.6750 - val_loss: 0.5383 - val_acc: 0.8228 - val_accuracy: 0.5865\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.59743\n",
            "Epoch 37/50\n",
            "5072/5072 [==============================] - 546s 108ms/step - loss: 0.4025 - acc: 0.8622 - accuracy: 0.6771 - val_loss: 0.5342 - val_acc: 0.8215 - val_accuracy: 0.5832\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.59743\n",
            "Epoch 38/50\n",
            "5072/5072 [==============================] - 544s 107ms/step - loss: 0.4036 - acc: 0.8618 - accuracy: 0.6762 - val_loss: 0.5490 - val_acc: 0.8196 - val_accuracy: 0.5789\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.59743\n",
            "Epoch 39/50\n",
            "5072/5072 [==============================] - 548s 108ms/step - loss: 0.4009 - acc: 0.8623 - accuracy: 0.6773 - val_loss: 0.5322 - val_acc: 0.8258 - val_accuracy: 0.5933\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.59743\n",
            "Epoch 40/50\n",
            "5072/5072 [==============================] - 541s 107ms/step - loss: 0.4031 - acc: 0.8615 - accuracy: 0.6758 - val_loss: 0.5398 - val_acc: 0.8241 - val_accuracy: 0.5894\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.59743\n",
            "Epoch 41/50\n",
            "5072/5072 [==============================] - 543s 107ms/step - loss: 0.3983 - acc: 0.8642 - accuracy: 0.6819 - val_loss: 0.6151 - val_acc: 0.8076 - val_accuracy: 0.5510\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.59743\n",
            "Epoch 42/50\n",
            "5072/5072 [==============================] - 544s 107ms/step - loss: 0.4032 - acc: 0.8622 - accuracy: 0.6773 - val_loss: 0.5553 - val_acc: 0.8215 - val_accuracy: 0.5833\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.59743\n",
            "Epoch 43/50\n",
            "5072/5072 [==============================] - 544s 107ms/step - loss: 0.3941 - acc: 0.8648 - accuracy: 0.6834 - val_loss: 0.5631 - val_acc: 0.8231 - val_accuracy: 0.5871\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.59743\n",
            "Epoch 44/50\n",
            "5072/5072 [==============================] - 545s 107ms/step - loss: 0.4139 - acc: 0.8590 - accuracy: 0.6699 - val_loss: 0.6291 - val_acc: 0.8069 - val_accuracy: 0.5492\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.59743\n",
            "Epoch 45/50\n",
            "5072/5072 [==============================] - 545s 107ms/step - loss: 0.3952 - acc: 0.8653 - accuracy: 0.6844 - val_loss: 0.5478 - val_acc: 0.8252 - val_accuracy: 0.5921\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.59743\n",
            "Epoch 46/50\n",
            "5072/5072 [==============================] - 543s 107ms/step - loss: 0.3903 - acc: 0.8659 - accuracy: 0.6859 - val_loss: 0.5517 - val_acc: 0.8196 - val_accuracy: 0.5790\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.59743\n",
            "Epoch 47/50\n",
            "5072/5072 [==============================] - 545s 107ms/step - loss: 0.3881 - acc: 0.8664 - accuracy: 0.6869 - val_loss: 0.5435 - val_acc: 0.8215 - val_accuracy: 0.5834\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.59743\n",
            "Epoch 48/50\n",
            "5072/5072 [==============================] - 545s 108ms/step - loss: 0.3875 - acc: 0.8670 - accuracy: 0.6884 - val_loss: 0.6382 - val_acc: 0.8082 - val_accuracy: 0.5525\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.59743\n",
            "Epoch 49/50\n",
            "5072/5072 [==============================] - 542s 107ms/step - loss: 0.4304 - acc: 0.8547 - accuracy: 0.6604 - val_loss: 0.5630 - val_acc: 0.8260 - val_accuracy: 0.5939\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.59743\n",
            "Epoch 50/50\n",
            "5072/5072 [==============================] - 540s 106ms/step - loss: 0.3903 - acc: 0.8669 - accuracy: 0.6883 - val_loss: 0.5384 - val_acc: 0.8269 - val_accuracy: 0.5959\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.59743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f742450f668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "SVVqLQ-_4o0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2060
        },
        "outputId": "34a50978-a9dc-4b0f-e592-aa0925f147f8"
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = \"nadam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\", accuracy])\n",
        "\n",
        "\n",
        "filepath=\"weights_model_128_LSTM_1gram.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "early_stopping_monitor = EarlyStopping(patience=10)\n",
        "\n",
        "# Splitting the data for train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_input_data, train_target_data, test_size = .1, random_state = 0)\n",
        "model.fit(X_train,  y_train, batch_size = 210, epochs = 50, validation_data = (X_val, y_val), callbacks=callbacks_list, verbose = 1) \n",
        "\n",
        "files.download(\"weights_model_128_LSTM_1gram.best.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5072 samples, validate on 564 samples\n",
            "Epoch 1/50\n",
            "5072/5072 [==============================] - 544s 107ms/step - loss: 0.3723 - acc: 0.8723 - accuracy: 0.7009 - val_loss: 0.5673 - val_acc: 0.8210 - val_accuracy: 0.5821\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.58211, saving model to weights_model_128_LSTM_1gram.best.hdf5\n",
            "Epoch 2/50\n",
            "5072/5072 [==============================] - 538s 106ms/step - loss: 0.3704 - acc: 0.8730 - accuracy: 0.7024 - val_loss: 0.5817 - val_acc: 0.8161 - val_accuracy: 0.5707\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.58211\n",
            "Epoch 3/50\n",
            "5072/5072 [==============================] - 532s 105ms/step - loss: 0.3710 - acc: 0.8723 - accuracy: 0.7011 - val_loss: 0.5689 - val_acc: 0.8192 - val_accuracy: 0.5780\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.58211\n",
            "Epoch 4/50\n",
            "5072/5072 [==============================] - 535s 106ms/step - loss: 0.3647 - acc: 0.8747 - accuracy: 0.7064 - val_loss: 0.5911 - val_acc: 0.8135 - val_accuracy: 0.5647\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.58211\n",
            "Epoch 5/50\n",
            "5072/5072 [==============================] - 537s 106ms/step - loss: 0.3632 - acc: 0.8747 - accuracy: 0.7065 - val_loss: 0.6867 - val_acc: 0.8009 - val_accuracy: 0.5352\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.58211\n",
            "Epoch 6/50\n",
            "5072/5072 [==============================] - 536s 106ms/step - loss: 0.3704 - acc: 0.8728 - accuracy: 0.7017 - val_loss: 0.5750 - val_acc: 0.8236 - val_accuracy: 0.5883\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.58211 to 0.58832, saving model to weights_model_128_LSTM_1gram.best.hdf5\n",
            "Epoch 7/50\n",
            "5072/5072 [==============================] - 534s 105ms/step - loss: 0.3617 - acc: 0.8754 - accuracy: 0.7081 - val_loss: 0.6234 - val_acc: 0.8114 - val_accuracy: 0.5597\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.58832\n",
            "Epoch 8/50\n",
            "5072/5072 [==============================] - 533s 105ms/step - loss: 0.3591 - acc: 0.8758 - accuracy: 0.7091 - val_loss: 0.5731 - val_acc: 0.8203 - val_accuracy: 0.5804\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.58832\n",
            "Epoch 9/50\n",
            "5072/5072 [==============================] - 534s 105ms/step - loss: 0.3616 - acc: 0.8753 - accuracy: 0.7079 - val_loss: 0.5774 - val_acc: 0.8186 - val_accuracy: 0.5766\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.58832\n",
            "Epoch 10/50\n",
            "5072/5072 [==============================] - 531s 105ms/step - loss: 0.3928 - acc: 0.8663 - accuracy: 0.6871 - val_loss: 0.5721 - val_acc: 0.8221 - val_accuracy: 0.5849\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.58832\n",
            "Epoch 11/50\n",
            "5072/5072 [==============================] - 536s 106ms/step - loss: 0.3614 - acc: 0.8764 - accuracy: 0.7105 - val_loss: 0.5697 - val_acc: 0.8239 - val_accuracy: 0.5890\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.58832 to 0.58896, saving model to weights_model_128_LSTM_1gram.best.hdf5\n",
            "Epoch 12/50\n",
            "5072/5072 [==============================] - 536s 106ms/step - loss: 0.3560 - acc: 0.8772 - accuracy: 0.7124 - val_loss: 0.5722 - val_acc: 0.8200 - val_accuracy: 0.5798\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.58896\n",
            "Epoch 13/50\n",
            "5072/5072 [==============================] - 535s 106ms/step - loss: 0.3571 - acc: 0.8765 - accuracy: 0.7109 - val_loss: 0.5664 - val_acc: 0.8188 - val_accuracy: 0.5771\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.58896\n",
            "Epoch 14/50\n",
            "5072/5072 [==============================] - 538s 106ms/step - loss: 0.3633 - acc: 0.8749 - accuracy: 0.7069 - val_loss: 0.5773 - val_acc: 0.8239 - val_accuracy: 0.5889\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.58896\n",
            "Epoch 15/50\n",
            "5072/5072 [==============================] - 542s 107ms/step - loss: 0.3548 - acc: 0.8775 - accuracy: 0.7130 - val_loss: 0.5664 - val_acc: 0.8254 - val_accuracy: 0.5925\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.58896 to 0.59249, saving model to weights_model_128_LSTM_1gram.best.hdf5\n",
            "Epoch 16/50\n",
            "5072/5072 [==============================] - 542s 107ms/step - loss: 0.3718 - acc: 0.8722 - accuracy: 0.7007 - val_loss: 0.5988 - val_acc: 0.8193 - val_accuracy: 0.5783\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.59249\n",
            "Epoch 17/50\n",
            "5072/5072 [==============================] - 542s 107ms/step - loss: 0.3571 - acc: 0.8773 - accuracy: 0.7125 - val_loss: 0.6999 - val_acc: 0.7989 - val_accuracy: 0.5307\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.59249\n",
            "Epoch 18/50\n",
            "5072/5072 [==============================] - 542s 107ms/step - loss: 0.3999 - acc: 0.8654 - accuracy: 0.6849 - val_loss: 0.8738 - val_acc: 0.7888 - val_accuracy: 0.5072\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.59249\n",
            "Epoch 19/50\n",
            "5072/5072 [==============================] - 541s 107ms/step - loss: 0.3758 - acc: 0.8733 - accuracy: 0.7029 - val_loss: 0.5909 - val_acc: 0.8261 - val_accuracy: 0.5941\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.59249 to 0.59405, saving model to weights_model_128_LSTM_1gram.best.hdf5\n",
            "Epoch 20/50\n",
            "5072/5072 [==============================] - 540s 107ms/step - loss: 0.3549 - acc: 0.8784 - accuracy: 0.7151 - val_loss: 0.6571 - val_acc: 0.8069 - val_accuracy: 0.5492\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.59405\n",
            "Epoch 21/50\n",
            "5072/5072 [==============================] - 541s 107ms/step - loss: 0.3585 - acc: 0.8769 - accuracy: 0.7116 - val_loss: 0.5863 - val_acc: 0.8191 - val_accuracy: 0.5777\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.59405\n",
            "Epoch 22/50\n",
            "5072/5072 [==============================] - 541s 107ms/step - loss: 0.3497 - acc: 0.8792 - accuracy: 0.7170 - val_loss: 0.5828 - val_acc: 0.8205 - val_accuracy: 0.5809\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.59405\n",
            "Epoch 23/50\n",
            "5072/5072 [==============================] - 541s 107ms/step - loss: 0.3503 - acc: 0.8786 - accuracy: 0.7157 - val_loss: 0.6122 - val_acc: 0.8138 - val_accuracy: 0.5654\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.59405\n",
            "Epoch 24/50\n",
            "5072/5072 [==============================] - 541s 107ms/step - loss: 0.3543 - acc: 0.8777 - accuracy: 0.7134 - val_loss: 0.5735 - val_acc: 0.8170 - val_accuracy: 0.5728\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.59405\n",
            "Epoch 25/50\n",
            "5072/5072 [==============================] - 535s 106ms/step - loss: 0.3493 - acc: 0.8790 - accuracy: 0.7165 - val_loss: 0.5728 - val_acc: 0.8232 - val_accuracy: 0.5872\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.59405\n",
            "Epoch 26/50\n",
            "5072/5072 [==============================] - 527s 104ms/step - loss: 0.3465 - acc: 0.8799 - accuracy: 0.7185 - val_loss: 0.6369 - val_acc: 0.8070 - val_accuracy: 0.5494\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.59405\n",
            "Epoch 27/50\n",
            "5072/5072 [==============================] - 526s 104ms/step - loss: 0.3670 - acc: 0.8740 - accuracy: 0.7046 - val_loss: 0.5825 - val_acc: 0.8256 - val_accuracy: 0.5929\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.59405\n",
            "Epoch 28/50\n",
            "5072/5072 [==============================] - 526s 104ms/step - loss: 0.3505 - acc: 0.8789 - accuracy: 0.7162 - val_loss: 0.6018 - val_acc: 0.8111 - val_accuracy: 0.5590\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.59405\n",
            "Epoch 29/50\n",
            "5072/5072 [==============================] - 534s 105ms/step - loss: 0.3538 - acc: 0.8778 - accuracy: 0.7137 - val_loss: 0.5864 - val_acc: 0.8231 - val_accuracy: 0.5872\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.59405\n",
            "Epoch 30/50\n",
            " 630/5072 [==>...........................] - ETA: 7:25 - loss: 0.3524 - acc: 0.8787 - accuracy: 0.7139"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cQaoXWr_VR-Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3975
        },
        "outputId": "5e422542-dc26-460e-bc2e-55545aa97759"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "model.compile(optimizer = \"nadam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\", accuracy])\n",
        "\n",
        "\n",
        "filepath=\"weights_model3_128_LSTM_1gram.best_one.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "early_stopping_monitor = EarlyStopping(patience=10)\n",
        "\n",
        "# Splitting the data for train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_input_data, train_target_data, test_size = .1, random_state = 0)\n",
        "model.fit(X_train,  y_train, batch_size = 210, epochs = 50, validation_data = (X_val, y_val), callbacks=callbacks_list, verbose = 1) \n",
        "\n",
        "files.download(\"weights_model3_128_LSTM_1gram.best_one.hdf5\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5072 samples, validate on 564 samples\n",
            "Epoch 1/50\n",
            "5072/5072 [==============================] - 596s 118ms/step - loss: 0.3719 - acc: 0.8723 - accuracy: 0.7010 - val_loss: 0.5661 - val_acc: 0.8197 - val_accuracy: 0.5790\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.57903, saving model to weights_model3_128_LSTM_1gram.best_one.hdf5\n",
            "Epoch 2/50\n",
            "5072/5072 [==============================] - 614s 121ms/step - loss: 0.3687 - acc: 0.8733 - accuracy: 0.7032 - val_loss: 0.5952 - val_acc: 0.8179 - val_accuracy: 0.5750\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.57903\n",
            "Epoch 3/50\n",
            "5072/5072 [==============================] - 625s 123ms/step - loss: 0.3674 - acc: 0.8737 - accuracy: 0.7041 - val_loss: 0.5893 - val_acc: 0.8147 - val_accuracy: 0.5677\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.57903\n",
            "Epoch 4/50\n",
            "5072/5072 [==============================] - 625s 123ms/step - loss: 0.3710 - acc: 0.8727 - accuracy: 0.7017 - val_loss: 0.5741 - val_acc: 0.8214 - val_accuracy: 0.5830\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.57903 to 0.58302, saving model to weights_model3_128_LSTM_1gram.best_one.hdf5\n",
            "Epoch 5/50\n",
            "5072/5072 [==============================] - 629s 124ms/step - loss: 0.3656 - acc: 0.8743 - accuracy: 0.7056 - val_loss: 0.5682 - val_acc: 0.8254 - val_accuracy: 0.5924\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.58302 to 0.59244, saving model to weights_model3_128_LSTM_1gram.best_one.hdf5\n",
            "Epoch 6/50\n",
            "5072/5072 [==============================] - 625s 123ms/step - loss: 0.3636 - acc: 0.8745 - accuracy: 0.7060 - val_loss: 0.6031 - val_acc: 0.8124 - val_accuracy: 0.5621\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.59244\n",
            "Epoch 7/50\n",
            "5072/5072 [==============================] - 622s 123ms/step - loss: 0.4348 - acc: 0.8549 - accuracy: 0.6604 - val_loss: 0.5920 - val_acc: 0.8220 - val_accuracy: 0.5847\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.59244\n",
            "Epoch 8/50\n",
            "5072/5072 [==============================] - 601s 118ms/step - loss: 0.3717 - acc: 0.8744 - accuracy: 0.7059 - val_loss: 0.6127 - val_acc: 0.8166 - val_accuracy: 0.5719\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.59244\n",
            "Epoch 9/50\n",
            "5072/5072 [==============================] - 596s 117ms/step - loss: 0.3666 - acc: 0.8750 - accuracy: 0.7072 - val_loss: 0.5690 - val_acc: 0.8253 - val_accuracy: 0.5923\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.59244\n",
            "Epoch 10/50\n",
            "5072/5072 [==============================] - 603s 119ms/step - loss: 0.3628 - acc: 0.8754 - accuracy: 0.7082 - val_loss: 0.5828 - val_acc: 0.8231 - val_accuracy: 0.5870\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.59244\n",
            "Epoch 11/50\n",
            "5072/5072 [==============================] - 605s 119ms/step - loss: 0.3578 - acc: 0.8766 - accuracy: 0.7110 - val_loss: 0.6079 - val_acc: 0.8151 - val_accuracy: 0.5682\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.59244\n",
            "Epoch 12/50\n",
            "5072/5072 [==============================] - 604s 119ms/step - loss: 0.3544 - acc: 0.8775 - accuracy: 0.7130 - val_loss: 0.5864 - val_acc: 0.8161 - val_accuracy: 0.5707\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.59244\n",
            "Epoch 13/50\n",
            "5072/5072 [==============================] - 608s 120ms/step - loss: 0.3615 - acc: 0.8754 - accuracy: 0.7080 - val_loss: 0.6523 - val_acc: 0.8076 - val_accuracy: 0.5509\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.59244\n",
            "Epoch 14/50\n",
            "5072/5072 [==============================] - 606s 120ms/step - loss: 0.3598 - acc: 0.8762 - accuracy: 0.7097 - val_loss: 0.5729 - val_acc: 0.8229 - val_accuracy: 0.5866\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.59244\n",
            "Epoch 15/50\n",
            "5072/5072 [==============================] - 609s 120ms/step - loss: 0.3530 - acc: 0.8778 - accuracy: 0.7138 - val_loss: 0.5857 - val_acc: 0.8239 - val_accuracy: 0.5889\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.59244\n",
            "Epoch 16/50\n",
            "5072/5072 [==============================] - 612s 121ms/step - loss: 0.3540 - acc: 0.8773 - accuracy: 0.7125 - val_loss: 0.5813 - val_acc: 0.8206 - val_accuracy: 0.5811\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.59244\n",
            "Epoch 17/50\n",
            "5072/5072 [==============================] - 609s 120ms/step - loss: 0.3541 - acc: 0.8773 - accuracy: 0.7126 - val_loss: 0.6328 - val_acc: 0.8107 - val_accuracy: 0.5581\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.59244\n",
            "Epoch 18/50\n",
            "5072/5072 [==============================] - 610s 120ms/step - loss: 0.3587 - acc: 0.8761 - accuracy: 0.7096 - val_loss: 0.5917 - val_acc: 0.8179 - val_accuracy: 0.5749\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.59244\n",
            "Epoch 19/50\n",
            "5072/5072 [==============================] - 612s 121ms/step - loss: 0.3609 - acc: 0.8755 - accuracy: 0.7083 - val_loss: 0.5706 - val_acc: 0.8248 - val_accuracy: 0.5910\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.59244\n",
            "Epoch 20/50\n",
            "5072/5072 [==============================] - 616s 121ms/step - loss: 0.3504 - acc: 0.8789 - accuracy: 0.7162 - val_loss: 0.5703 - val_acc: 0.8221 - val_accuracy: 0.5846\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.59244\n",
            "Epoch 21/50\n",
            "5072/5072 [==============================] - 618s 122ms/step - loss: 0.3551 - acc: 0.8770 - accuracy: 0.7119 - val_loss: 0.5686 - val_acc: 0.8203 - val_accuracy: 0.5805\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.59244\n",
            "Epoch 22/50\n",
            "5072/5072 [==============================] - 620s 122ms/step - loss: 0.3545 - acc: 0.8773 - accuracy: 0.7125 - val_loss: 0.7602 - val_acc: 0.7955 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.59244\n",
            "Epoch 23/50\n",
            "5072/5072 [==============================] - 617s 122ms/step - loss: 0.4698 - acc: 0.8464 - accuracy: 0.6403 - val_loss: 0.6296 - val_acc: 0.8137 - val_accuracy: 0.5653\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.59244\n",
            "Epoch 24/50\n",
            "5072/5072 [==============================] - 614s 121ms/step - loss: 0.3703 - acc: 0.8760 - accuracy: 0.7096 - val_loss: 0.6051 - val_acc: 0.8227 - val_accuracy: 0.5861\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.59244\n",
            "Epoch 25/50\n",
            "5072/5072 [==============================] - 614s 121ms/step - loss: 0.3555 - acc: 0.8790 - accuracy: 0.7164 - val_loss: 0.5748 - val_acc: 0.8242 - val_accuracy: 0.5897\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.59244\n",
            "Epoch 26/50\n",
            "5072/5072 [==============================] - 613s 121ms/step - loss: 0.3513 - acc: 0.8793 - accuracy: 0.7172 - val_loss: 0.5766 - val_acc: 0.8194 - val_accuracy: 0.5784\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.59244\n",
            "Epoch 27/50\n",
            "5072/5072 [==============================] - 612s 121ms/step - loss: 0.3485 - acc: 0.8798 - accuracy: 0.7183 - val_loss: 0.6628 - val_acc: 0.8059 - val_accuracy: 0.5469\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.59244\n",
            "Epoch 28/50\n",
            "5072/5072 [==============================] - 612s 121ms/step - loss: 0.3712 - acc: 0.8730 - accuracy: 0.7026 - val_loss: 0.6320 - val_acc: 0.8150 - val_accuracy: 0.5682\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.59244\n",
            "Epoch 29/50\n",
            "5072/5072 [==============================] - 614s 121ms/step - loss: 0.3530 - acc: 0.8790 - accuracy: 0.7164 - val_loss: 0.5678 - val_acc: 0.8259 - val_accuracy: 0.5935\n",
            "\n",
            "Epoch 00029: val_accuracy improved from 0.59244 to 0.59345, saving model to weights_model3_128_LSTM_1gram.best_one.hdf5\n",
            "Epoch 30/50\n",
            "5072/5072 [==============================] - 615s 121ms/step - loss: 0.3462 - acc: 0.8803 - accuracy: 0.7196 - val_loss: 0.5883 - val_acc: 0.8173 - val_accuracy: 0.5734\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.59345\n",
            "Epoch 31/50\n",
            "5072/5072 [==============================] - 613s 121ms/step - loss: 0.3443 - acc: 0.8808 - accuracy: 0.7208 - val_loss: 0.6362 - val_acc: 0.8066 - val_accuracy: 0.5485\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.59345\n",
            "Epoch 32/50\n",
            "5072/5072 [==============================] - 615s 121ms/step - loss: 0.3458 - acc: 0.8800 - accuracy: 0.7189 - val_loss: 0.5830 - val_acc: 0.8155 - val_accuracy: 0.5693\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.59345\n",
            "Epoch 33/50\n",
            "5072/5072 [==============================] - 604s 119ms/step - loss: 0.3463 - acc: 0.8799 - accuracy: 0.7188 - val_loss: 0.5905 - val_acc: 0.8130 - val_accuracy: 0.5635\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.59345\n",
            "Epoch 34/50\n",
            "5072/5072 [==============================] - 607s 120ms/step - loss: 0.3487 - acc: 0.8791 - accuracy: 0.7167 - val_loss: 0.5748 - val_acc: 0.8178 - val_accuracy: 0.5747\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.59345\n",
            "Epoch 35/50\n",
            "5072/5072 [==============================] - 604s 119ms/step - loss: 0.3504 - acc: 0.8788 - accuracy: 0.7161 - val_loss: 0.5775 - val_acc: 0.8233 - val_accuracy: 0.5875\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.59345\n",
            "Epoch 36/50\n",
            "5072/5072 [==============================] - 600s 118ms/step - loss: 0.3440 - acc: 0.8809 - accuracy: 0.7210 - val_loss: 0.5806 - val_acc: 0.8222 - val_accuracy: 0.5851\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.59345\n",
            "Epoch 37/50\n",
            "5072/5072 [==============================] - 600s 118ms/step - loss: 0.3443 - acc: 0.8806 - accuracy: 0.7203 - val_loss: 0.6027 - val_acc: 0.8220 - val_accuracy: 0.5845\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.59345\n",
            "Epoch 38/50\n",
            "5072/5072 [==============================] - 596s 118ms/step - loss: 0.3570 - acc: 0.8771 - accuracy: 0.7120 - val_loss: 0.5889 - val_acc: 0.8264 - val_accuracy: 0.5949\n",
            "\n",
            "Epoch 00038: val_accuracy improved from 0.59345 to 0.59491, saving model to weights_model3_128_LSTM_1gram.best_one.hdf5\n",
            "Epoch 39/50\n",
            "5072/5072 [==============================] - 597s 118ms/step - loss: 0.3427 - acc: 0.8812 - accuracy: 0.7218 - val_loss: 0.5934 - val_acc: 0.8171 - val_accuracy: 0.5730\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.59491\n",
            "Epoch 40/50\n",
            "5072/5072 [==============================] - 593s 117ms/step - loss: 0.3407 - acc: 0.8816 - accuracy: 0.7227 - val_loss: 0.5807 - val_acc: 0.8221 - val_accuracy: 0.5848\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.59491\n",
            "Epoch 41/50\n",
            "5072/5072 [==============================] - 594s 117ms/step - loss: 0.3415 - acc: 0.8814 - accuracy: 0.7221 - val_loss: 0.5822 - val_acc: 0.8230 - val_accuracy: 0.5870\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.59491\n",
            "Epoch 42/50\n",
            "5072/5072 [==============================] - 600s 118ms/step - loss: 0.3428 - acc: 0.8811 - accuracy: 0.7215 - val_loss: 0.5859 - val_acc: 0.8223 - val_accuracy: 0.5853\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.59491\n",
            "Epoch 43/50\n",
            "5072/5072 [==============================] - 620s 122ms/step - loss: 0.3416 - acc: 0.8814 - accuracy: 0.7224 - val_loss: 0.6307 - val_acc: 0.8193 - val_accuracy: 0.5781\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.59491\n",
            "Epoch 44/50\n",
            "5072/5072 [==============================] - 621s 122ms/step - loss: 0.3420 - acc: 0.8814 - accuracy: 0.7220 - val_loss: 0.6274 - val_acc: 0.8082 - val_accuracy: 0.5523\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.59491\n",
            "Epoch 45/50\n",
            "5072/5072 [==============================] - 620s 122ms/step - loss: 0.3494 - acc: 0.8795 - accuracy: 0.7174 - val_loss: 0.5966 - val_acc: 0.8194 - val_accuracy: 0.5784\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.59491\n",
            "Epoch 46/50\n",
            "5072/5072 [==============================] - 617s 122ms/step - loss: 0.3394 - acc: 0.8823 - accuracy: 0.7244 - val_loss: 0.6166 - val_acc: 0.8185 - val_accuracy: 0.5763\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.59491\n",
            "Epoch 47/50\n",
            "5072/5072 [==============================] - 613s 121ms/step - loss: 0.3389 - acc: 0.8824 - accuracy: 0.7244 - val_loss: 0.5762 - val_acc: 0.8234 - val_accuracy: 0.5877\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.59491\n",
            "Epoch 48/50\n",
            "5072/5072 [==============================] - 613s 121ms/step - loss: 0.3373 - acc: 0.8826 - accuracy: 0.7249 - val_loss: 0.5862 - val_acc: 0.8201 - val_accuracy: 0.5802\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.59491\n",
            "Epoch 49/50\n",
            "5072/5072 [==============================] - 606s 119ms/step - loss: 0.3374 - acc: 0.8826 - accuracy: 0.7250 - val_loss: 0.5939 - val_acc: 0.8257 - val_accuracy: 0.5932\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.59491\n",
            "Epoch 50/50\n",
            "5072/5072 [==============================] - 617s 122ms/step - loss: 0.3439 - acc: 0.8809 - accuracy: 0.7211 - val_loss: 0.5981 - val_acc: 0.8203 - val_accuracy: 0.5807\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.59491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-45366a179963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m210\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights_model3_128_LSTM_1gram.best_one.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Une erreur SSL sest produite et il est impossible dtablir une connexion scurise avec le serveur."
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6bE8hfV_Hp1p"
      },
      "cell_type": "markdown",
      "source": [
        "# Update to the best model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3Fqu1XFQHqY1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"gdrive/My Drive/weights_model_128_LSTM_1gram.final.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yG_-b8DsHkOM"
      },
      "cell_type": "markdown",
      "source": [
        "# Output the result "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k9tYt6spSVL2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Defining the decoders so that we can\n",
        "revsere_decoder_index = {value:key for key,value in tokenizer_decoder.word_index.items()}\n",
        "revsere_encoder_index = {value:key for key,value in tokenizer_encoder.word_index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "msZhKkmcH99r",
        "outputId": "d6bbb29f-fd0c-4f15-c9b6-584c5bc06613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2077
        }
      },
      "cell_type": "code",
      "source": [
        "y_test_pred = model.predict(test_input_data[:])\n",
        "print(len(test_input_data))\n",
        "for i in range(len(test_input_data)):\n",
        "    print_results(test_input_seqs[i], y_test_pred[i], revsere_decoder_index)\n",
        "\n",
        "kaggle_csv(model,test_input_data,test_df)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "119\n",
            "----EEEEEE-GGG-TEEE-TTSEEEEEEEETTTEEEEEETTTEEEEE-TT-TT----EEEEHHHHHHHHHTT--HHHHHHHH-TTSEEEEEETT---S-HHHHHEEETTEEEETTEEEEEEHHHH-HEEEETTTEEEEEEE-HHHHHHHHHHHHHHHTT---HHHHHHHHHHHHHHHHHHHHHHHHH-\n",
            "-EEEEEEEE--TTHHHHHHHHHHHHHHHHHHHHHTT-EEEEEEEEETTTEEEEEE-TT-EEEEEE-SHHHHHHHHHHSHHHHHHHHHHTT-EEEEEEEEE---\n",
            "-EEEEETTT-EEEEEE-TTT----SEEEEE-HHHHHHHH----GGEEEEEES----EEEEEEEEEEEEEEEEEE-----EEEEEEEEETEEEETT-EEEEEEEEEEEGEEEEEEEEEETTTEEEEEEEEETTEEE-\n",
            "--EEEEE--HHHHHHHHHTT--EEEEEE--GTTT--HHHHHHHHHHHH----EE--EEE---TTT--HHHHHHHHHHHHHHHHHT--EEEEEEEE--STT-HHHHHHHHHHHSSEEEEEE--TTTTS-HHHHHHHHHHTT-EEEEEES---THHHHHHHHHHHHHHTTS-EEEEEETT-TTHHHHHHHTT-EEEEE-----------G----EEEEEHHHHHHHHHHHHHT----EEEEE-SSHHHHHHHHHTT-SEEEEE---TTTT--HHHHHHHHHHHH--------EEE---TTT--HHHHHHHHHHHHHHHHTT--EEEEEEEE--S-T-HHHHHHHHHHHSSEEEEEE--TTTTS-HHHHHHHHHHTT-EEEEEES---THHHHHHHHHHHHHHTTS-EEEEEETT-TTHHHHHHHTT-EEEEE-----------GGGHHHH---HHHHHHHHHHHHHHHHH-\n",
            "-EEEEEEETHHHHHHHHHHHHTT-EEEEEESSS-HHHHHHHHTT---TTTEEEETTEEEE-HHHHHHHHTT-SEEEEEE-TTTTHHHHHHHGGGH-TTEEEEEESS---G-GGH--HHHHHHHHHHHHTTTEEEEE-HHHHHHHHHH---EEEEE-TTT--HHHHHHHHHHSSTT-EEEEEEEHHHHHHHHTT-EEEEEEEEHHHT-TT-EEEEEHTHHHHHHHHHHHHHHHHHHT---EEEEEEEETT-EEEEETT-HHHHHHHHHHTT--HHHHHHHHHHHT-EEEEE-S-HHHHHHH-HH----HHHHHHHHHHHHTT---HHHHHHHHHHHHHHHHHTTT-HHHHHHHHHHHTT-EEEEEEES--HHHHHHHHTT---TTTEEEETTEEEE-HHHHHHHHTT-SEEEEEE-TTTTHHHHHHHGGGH-TTEEEEEESS---G-GHH--HHHHHHHHHHHHHTTEEEEE-HHHHHHHHHH--TEEEEE-TTT--HHHHHHHHHHH----EEEEEEEHHHHHHHHTT-EEEEEEEHHHHHTTT-EEEEEEEEEEEHHHHHHHHHHHHHHT--EEEEEEEETTTEEEEEETT-HHHHHHHHHHTT--HHHHHHHHHHHT-EEEEE--HHHHHHHH-ST-----HHHHHHHHHHHTT---HHEEEEE----\n",
            "-----------TT----EEEEETT-EEEEEE--------EEE-TTT-EEEE-HHHHHHHHHHHHHHHHTT--EEEEE-TT--TT-EE-----HHHHT---EEEEEE----EEEEE---GGGGGEEEETTSEEETTTHHHHHHHHHHHHHT-TT--EEEE-HHHHHHHHHHTT--HHHHHHHHHGGGT-EEEEEEEE--TT-----T----TT-----EEEE-------TT-HHHHHTT--EEE--E--TT-----EEE-TT-EEEEEEE-----------TTT--EE--HHHHHHHHHHHHHHHHTT--EEEEE-TT--TT-EE-----HHHHT---EEEEEEEEEEEEEEE---GGGGGEEEETTSEEEETTHHHHHHHHHHHHHH-TT--EEEE-HHHHHHHHHHHT--HHHHHHHHHHHHH-EEEEEEEE--GGG----T----TT------EEEET-S----TT-HHHHHH-\n",
            "--------TT---S----EEEEEETEEEEEEEE-TTSEEEEEEEEEE--TT--HHHHHHHHHHHHH-TT-----EEEEEEEEETTEEEEE-GGGE-TT--TS--EEEEEEEEEEEEEEEEE-TTSEEEEEEEEEE--TT--HHHHHHHHHHHHH-TT------EEEEEEEETTEEEE-\n",
            "---EEEEETS-HHHHHHHHHHHHHHHHTT-EEEEEEEEETT--SS-T--HHHHHHHHHHHHHHHHH-TT-SEEEEEEEE--TT-EEEEEEEEEEEEETEEEEE-S---EEEEEH--TT--HHHHHGG-TT-EEEEE-TT-EEEEEEEEEESSSEHHHHHHHHH-TTT--\n",
            "----EEEEEETEEEE-TT-EEEEEEEEETE-E-EEEEEETS----TT-EEHHHHHTTTEEEEEEEEEETT-EEEEEEEEE-TTT-EEEEEEEEEE-\n",
            "-T--TTT---HHHHHHHHHHHHHHHHHHHHHT-TTTTTT--------EEEEEE--HHHHHHHHHHHHEEEEE-----TTTT--EEEEEESSHHHHHHHHHHHHHHHHHHHHHT----EEEEEEEEEEE-T---EEEEEEEEEETEEEEHTTTHHHHHHHH-TT--EEEE-E--TT----T-TTTGGTT-----\n",
            "--HHHHHHHHH-TTTTTHHHHHHHHHT-TTEEETTTEEEEEEETGGEEEEEE----TTS-HHHHHHHHHHHHHHHT-EEEEEE-HHHHHHHHHHHHHHHHHTTTT--ETTHHHHHT-TT-EEEEEEEEETS-EEEE--TT-HHHHHHHHHHHHHHHHHHHH----EHHHHT--TTS--HHHHHHHEHETTEE-HHHHHHTHHEEEET--HHHHHHHHHHHHHHHHHHHT------------HHHHHHHHHHHHHHHHHTT-EEE-TTT---HHHHHHHHHHHHHHHHHHHHHHHHTT--HHHHHHHHHTTSHHHHHHHHHHTTHT-HHHHHHHHHT-TT-TTTTTEEEEEETTGGGGGEEE----TTS-HHHHHHHHHHHHHHHT-EEEEEE-HHHHHHHHHHHHHHHHHTTTT--ETTHHHHHT-TTEEEEEEEEEETS-EEE---TTHHHHHHHHHHHHHHHHHHHHH----EEHHHT--TTS--HHHHHHHHHTTT---HHHHHHTHHHHEETT-HHHHHHHHHHHHHHHHHHHT------------HHHHHHHHHHHHHHHHHTT-EE--TTTT--HHHHHHHHHHHHHHHHHHHHHHHHTT--HHHHHHHHHTTSHHHHHHHHHHTTHT-HHHHHHHHHT-TT-TTTTT-EEEEETT-GEEEEEE----TTS--HHHHHHHHHHHHHHT-EEEEE\n",
            "----HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHTTG-HHHHHHHH-HHHHHHHHHHHHHHHHHHTT---HHHHHHHHHHHHTHHHHHHHHHHHHHHHHHHTT--HHHHHHHHHHHHHHTTT-HHHHHHHHHHHHHHHHHHHHHHHTHHHHHHHHHHHHTTTHHHHHHHHHHHHHHHHHHHHHHTTTTTT---T-HHHHHHHHHHHHHHHHHHHHT-THHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHTT-T-HHHHHHHHHHHHHHH-TT--EEEE-TEEEEHHHHTHHHHHHHHHHHHHH--T----TTT-HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH-EEEEETTHHHHHHHHHHH-HTT---\n",
            "----EEEEEEE-TT--HHHHHHHHHHHH-TEEEEEEEETTTTSSSTHHHHHHHHHHHT-SEEEEEE--GGGGG-----EEEEEETS-HHHHHHHHHHHHTTTEEEEEE-HHHHHHHHHHHHHHHT-EEEEEEE------HHGH--TT--HHHHHHHHHHH---HHEEEEETTTTTSSSHHHHHHHHHHHHT-EEEEEEE--GGGGTT----EEEEEETS-HHHHHHHHHHHTTTTEEEEEETTTTHHHHHHHHHHHHT--EEEEE--\n",
            "---TTSEEEEEE-TT------TT---EEHHHHHHTT-EEEE-S----T---TTEEEEEEE-HHHHH--HHH---HHHHHHHHHT-----HHHHHH---TT---HHHHHHHHHHHHHH---HHHHHHHHHHHT------HHHHHHHHHHHHHHHHHHHHHHHHHT----HHHHHH---GG-EE-TTTEEEEEEEE-TTG-HHHHHHHHHHHTT------HHHHHTHHHHHHHHH--TT---HHHHHHHHHHT--GG---HHHHHHHHHHHH-HHHHHHHHHHHHHHHHHHSS--EEEEE---HHHHHHHHHHHHHHT--TT--EEEEEE-GGGGGGGHHHHHHHHTHTTEEEEEEEETEEEEE--\n",
            "--HHHHHHHHHHHHH-TTTTT--HHHHHHHHHHHHHHT--HHHHHHHHHHHHHHHHTT----HHHHHHHHHHHHHHHHHH-\n",
            "----EEEEEEEEEEEETEEEEETTEEEEEHHHHHHHTTTTT--EHHHHHHHHHHHHHHHHHHTTEEE--TT----TT--HHHHHHHHHHHHHHHHSHHHHHHHHHHHTTT-EEEE--GG------ETTTTT-EHHHHHHHHHHT-TT--HHHHHHHHHHHHHHHTT--EEEEE-EETTTTT-HHHHHHHHHHTT-HHHHHHHHHHHHT--EE--HHHHHHHHHHTT-EEEEE-SGGGGGGT-TTHHHHHHHHHHHHHHHT-HHHHHHHHHHHHHHHHHHHHHHHHT--HHHHHHHHHHHHHHT---HHHHHHHH--TTT-HHHHHHHHHHHHH--HHHHHHHHHHHHHHHTTEEEEEEEE-TTS-------G---TT-EEEEEEEETTTEEEEEEEEEEEE-E-TT-EEEE-EEEEETTT--EEEEEEE---TTTEEEEEEEEEE-EEEEETT-EEEEE-TT-TT-EEEEEETTEE-TTTEEEEETTT-HHHHHHHHHHT-----TTEEEEEETTEEEHHHHHHHHTTT--EEEHHHHHHHHHHHHHHHHHHTT-----TT----TT--HHHHHHHHHHHHHHHHSHHHHHHHHHHHHTT--EEE---------EETTTTT-EHHHHHHHHHHT-TT--HHHHHHHHHHHHHHHHT--EEE-H-H-TTTTT-HHHHHHHHHHTT-H-HHHHHHHHHHHHH\n",
            "--EEEE--TT-EEEEEE-TGGEEETTTS-EEEE-TTT--EEEEEEEEE------EEETTTEEEEEETTT-EEEEEEES-E-GTTSEEEEEEEE-TTS-EEEEEEEEEEE-GHHHHHH-TT-EEEEEEEEGGEEE-TTS--EEE-TTT---EEEEEEE-------EEETTTEEEEE-TTT-EEEEEEES-EGGGSEEEEEEEEE-TTS-EEEEEEEEEE--\n",
            "--HHHHEEE--TTEEEEEEET--TTS--EEEEEEEEEEE-TT-EEEE-TT--EEHEEHHHHHHHH-HH-TT-H---HHHHHHHHHHHTS---HHHHHHHHHHHHHHHHHHHHHHHHHH-----STTS-EE-HHHHHHHHHHHTT--TTEEEEE-TTTTEEEEEEEEEEEEETTEEEEEEEEEEEEEEEE-\n",
            "--HHHHHHHHHHHHHTT-HHHHHHHHHHHHSTT--HHHHHHHHHHHH-HHHHHHHHHHHHHHHHHTHHHHHHHHHHHHHHHHHHHHHHHTT---HHHHHHHHHHSTT--HHHHHHHHHHHH-HHHHHHHHHTTHHHHTHTTHHHHHHH-\n",
            "--HHHHHTH-S-HHHHHHHHHHHH----HTT--GGGHHHHHH-HHHT---TT----HHHHHHT-\n",
            "-EEEEEEEEG--TTTEEEEEEETTTEEEEEEEE-GGGHHHTT--HHHHHT--HHHHHHHHHHHTT--EEEEETT-HHHHHHHHHT--SSEEEETTEEEE-TT--EEEEEETTT-THHHHHHHHH-SEEEET-S--EEEEEEETTEEEEEEEEEETS-EEEEEEEEEEEEEEEHHHHHHHHHHHHHHHHHHHHHTT---EEEEEEEETTEEEEEEEETTEEEEEEEEE---HHHHHHHHHHHHHHHHHT--GTT-EEEEEETTEEEEEEEE--GGGGHGTT--HHHHHT--HHHHHHHHHHHTT--EEEEETT-HHHHHHHHHT--SSEEEEETEEEE-------EEEEETTT-HHHHHHHHHHHHHHHHH-S--EEEEEEETTEEEEEEEEEETS-EEEEEEEEEEEEEEEHHHHHHHHHHHHHHHHHHHHHTT---EEEEEEEETTEEEEEEEETTEEEEEEEEE---HHHHHHHHHHHHHHHHHT--GTT-EEEEEETTEEEEEEEEE-GGGGGGTT--HHHHHT--HHHHHHHHHHHTT-SEEEEETT-THHHHHHHTT--SSEEEEETEEEE-------EEEEEEETTT-HHHHHHHHHHHHHHHHT----EEEEEEETTEEEEEEEEEETS-EEEEEEETTEEEEEEHHHHHHHHHHHHHHTHHHHHHTT---EEEEEEEETTEEEEEEEE-T--\n",
            "--EGGG--TT-EEEEETEEEEEEEEEEEEEEETEEEEEEEEETTTTTEEEEEEEETS-EEGEEEEEEEEEEEEETTT-EEEEEETEEEEEEEE-T-EEETGGEEETT-EEEEEEETT-EEEE---EEEEEEEEE--TT-EEEEEETTS--EEE-TT-EEEEEEE--TT-EEEEETTT--EEEEEEEEGGG--TT-EEEEEEEEEEEEEEEEEEEEETEEEEEEEEETTTTTEEEEEEEETS-EEGEEEEEEEEEEEEETTT-EEEEEETTEEEEEEE-T--EETGGGETTT-EEEEEEETT-EEEE---EEEEEEEEE--TTTTT----EE-TT-EEEEEEEE-TT-EEEEETTT-EEEEE-\n",
            "------EEEEEEEEEEEEEETTEEEEEEEEEE--TTTEEEEEE--TTT----HHHHHHHHHHHHHHHHHHTT--TTEEEEEEEE--T---TTSHHHHHHHHHHHHHHHSTTS-EEEEEHHHTT----EEEETTTT--TTT-HHHGGG-S--S-EEEEETT-------EEE---GGGG-TT--HHHHHHHHHTT-----E----HHHHHHHHHHHHHHHHHHHTT-EEEEEEEHHHHHHHHTT-HHHHHHHHHHHHHHHHHHHHEET--\n",
            "---EEEEE--TS-----S-HHHHHHHHHHHHHHHHHTT-EEEEEE----TTT-TTT----HHHHHHHHHHH--HHHHHHHHHHHHHTT------EEEEEEETTEEEEEEEEEEE-TTS-EEEEEEEEE-TT----EE-------------TT--T--EEEETTTTEEEEEETTTT-HHHHHHHT-TT-EEEEEEEE--T------------EEHHEEEHHHTHHHHHHHHHHHHHTT--TEEEEEEEEEEE-TT-EEEEEEEETTEEEEEEEEEHHHHHHHHHHHHHHHHH----EEEEEEEE-GEEEEEE--SSHHHHHHHHHHHHHHHHHHHHHHHHTT-EEEEEEEEE--TT-THT----HHHHHHHHHHH--HHHHHHHHHHHHHTT-----EEHEEEEETTEEEEEEEEEEE-TTS-EEEEEEEEE-TT----EEE------------TT--T--EEEETTTEEEEEEETTTT-HHHHHHHHHTT-EEEEEEEE-TT------------EHEEEEEEHHHHHHHHHHHHHHHHTT--TEEEEEEEEEEEETT-EEEEEEEEETEEEEEEEEEGGGHHHHHHHHHHHHGG----HH-HH-E-\n",
            "------HHHHHHHHHHHHHHHHHS---HHHHHHHHHHHHHHHHHHHHS-S--HHHHHHHHHHHHHHHHHS---HHHHHHHHHHHHHHHHHHHHHTT-----HHHHHHHHHHHHHHHHHS---HHHHHHHHHHHHHHHHHHHHTS----HHHHHHHHHHHHHHHHTS---HHHHHHHHHHIIHHHHHH-\n",
            "HHTT-TTS-SEEEEEE--TTT-HHHHHHHHHHHHTTTEEEEEEE--TT----S----GGS--HHHHHHHHHHT-HHHHHHHHHHHHHHHT--EEEEE-THHHHHHHHHHHTT---EEEEEEE--T------TT-----TT-EEEE---TT-TT--ETTEEEEEEETTTT-EE-HHHHHHHHHHH-----THHHHHHHHTT-TT--HHHHHHHT-EEEEHH---------EEEEHHHHHHHHTSTTS-SEEEEE---TTT-HHHHHHHHHHH--ETEEEEEEE--TT-------------HHHHHHHHHHHT-HHHHHHHHHHHHHHHT--EEEEE-THHHHHHHHHHHTT---EEEEEEE--T------TT-----TT-EEEE---TT-TT--ETTEEEEEEETTTT-EE-HHHHHHHHHHH-----THHHHHHHHTT-TT--HHHHHHHT-EEEEHH---------EEEEHHHHHHHHTSTTS-SEEEEE---TTT-HHHHHHHHHHH--ETEEEEEEE--TT-------------HHHHHHHHHHHT-HHHHHHHHHHHHHHHT--EEEEE-THHHHHHHHHHHTT---EEEEEEE--T------TT-----TT-EEEE---TT-TT-HHTTEEEEEEEE-TTT---HHHHHHHHHHH-----THHHHHHHHHT-TT--HHHHHHHT-EEEHHHHHH-\n",
            "--EEEESEEEEESS-HHHHHHHHHHHHHHHHHHTT---HHHHHHHHHH----TTGSS-HHHHHHHT---E-EE-EEEEE-TT-HHHHHHHHHHHHTS-HHHHHHHHHHHHHHHH-TTHH--\n",
            "--HHHHHHHHH--EEEEEEEEEE--HHHHHHHTT-----EEEEEEEETTTT-EEEEEEEE-TTT--EEEEEEETTS-EEEEEEE-HHHHHHHHH--EEEEEEEEEE--HHHHHHHTT----SEEEEEEEETTTT-EEEEEEEE-TTTS-EEEEEEETTS-EEEEEEE-HHHHHHHHH--EEEEEEEEEE-HHHHHHHHTT----S-EEEEEEETTTT-EEEEEEEE-TTTS-EEEEEEETTS-EEEEEE-HHHHHHHHH--EEEEEEEEEE-HHHHHHHHTT----SEEEEEEEETTTT-EEEEEEEE-TTTS-EEEEEEETTS-EEEEEEE-HHHHHHHHH--EEEEEEEEEE-HHHHHHHHTT----S-EEEEEEETTTT-EEEEEEEE-TTTS-EEEEEEETTS-EEEEEEE-HHHHHHHHH--EEEEEEEEEE-HHHHHHHHTT----S-EEEEEEETTTT-EEEEEEEE-TTTS-EEEEEEETTS-EEEEEEE-HHHHHHHHH--EEEEEEEEEE-HHHHHHHHTT----S-EEEEEEETTTT-EEEEEEEE-TTTS-EEEEEEETTS-EEEEEEE-HHHHHHHHH--EEEEEEEEEE--HHHHHHHTT---GG-EEEEEEETTTT-EEEEEEEE-TTT--EEEEEEETTS-EEEEE-\n",
            "-EEE-TS----EEEEEEEEETTTTTTEEEEEEETTS-EEEEEEEEEEEET---EEEEEEE-SEEEEEEEEEEE--EEEEEEEEEEE-TTS-EEEEEEEEESS-TEEEEETTEEEEEEEEEEEEETHHHHHHHHTT--EEEEEEEEE--\n",
            "-HHHHHHHHHH-TTT-E-TT-EEEEEETS-TTEEEEEEEE-TT----TTS----S-HHHHHHHHHHHTT--GGGEEEEE----------S---TT-EEE-HHHHHHHHHHH-TTHHHHHHHHHHHHHET--EEEEEEETEEEEETTEEEEE---GGGHHT-GGG-TT--G---EEEHHHHHHHHHHS-----\n",
            "--HHHHHHHHHHHHHHHHHHHHHHHHHHHHHTTT--HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH-T-HHHHHHHHHHHHHHHHHHHHHHHHHHHH-TT---HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH-\n",
            "--EEEEEETTTTTHHHHHHHHHHHHHT--G---HEEEEEEEEESEEEEEE-HHHHHHHHTT-EEEE-TTT--HHHHHHHHHHHHTT--EEE-EEE--------EEEEEE--SHHHHHHHHHHH-HHHHHHHTEEEEEET-HHHHHHHHHHHHHHHTT--HHHHHHHHHHHHHHHHHHHH----EEEEEEEESS-SHHHHHHHHHHHHHHHHTT--EEEEEE-TTTTHHHHHHHHHHHHHT--G---H-EEEEEEEESEEEEEE-HHHHHHHHTT-E-EE-TTT--HHHHHHHHHHHHTT--EEE-EEE--------EEEEEE-TSHHHHHHHHHHHHHHHHHHHT-EEEE-T-HHHHHHHHHHHHHHHTT--HHHHHHHHHHHHHHHHHHHHH-HHHHHHHH--TT-HHHHHHHHHHHHHHHHHT-\n",
            "--------T------HHHHHHHHHHH-TT--GGG--HHHHHHHHHHHTTTTT-HHHHHHHHHHHHHHHHHHH-\n",
            "-GGG--H--H-TTHHHHHHTH-HHHHHHHTTT-------H--TTTSS---HHHHHHHHHHHHHHHHTT-T-H-HEEEEEHHHEHTTEEEEEE-EEEEETT--HHHHHHHHHHHHHTT--EEEEEEEEEETT-EEEEEEETEEEEEE-TTTEEEEE-----HHHHHHHHHHHHHTT--EEEEEEEEEE-\n",
            "--EEEEEEE-TTTTTTT--EEEEEEEE-----------TTTT--HHHHHHHHHHHHHHHHHHHHHHHTT----EEEEEEEEEEEEETTEEEEEEEEEEEEEEETT--HHHHHHHHHHHHTT--HHHHHTT-EEEEEEEEEEEEE-HHHHHHHTHHTT----E---------T-------TTTT--HHHHHHHHHHHHHHHHHHHHHHHTT----EEEEEEEEEEEEETTEEEEEEEEEEEEEEETT--HHHHHHHHHHHHTT---EGG-TT-EEEEEEEEE-\n",
            "----EEEEEETEEEETTEEEEE---EEEEEETTS-EEEEEEEEE------E--T--EEEEEEEEEEEE--TTEEEEEEEETTEEEEEEEEE--HHHHHHHHHHHHHHHHHHHHHHH-\n",
            "------EEEEEEE--S----TTS-EEE-TTTTTEEEEEETTEEEEEEEE---EEEEEEETEEE-\n",
            "-------EEEEEEEE--HHHHHHHHHHHHHHHHHHHHHHT--EEEEEETTHHHHHHHHHHHTT--------EEEEEEEEEETTEEEEEEE--T------G-EEEEEETTEEEEEEEEETTHHEEE-HHHHHHHHHHHHHHTT-EEEEEEEGGHHHHHHHHHHHHHHT--EEEEEEEEEEEEETTTT--HHHHHHHHHHHHHHTT---HHHHHHHHHHHHHH----EEEEHHEEEEEES--HHHHHHHHHHHHHHHH-----EEEEEEETTHHHHHHHHHHHTT-EEEE---EEEEEEEEEETEEEEEEEE---------G-EEEEEETTEEEEEEEEETTTHEEE-HHHHHHHHHHHHHHTT-EEEEEEEGGHHHHHHHHHHHHHHT--EEEEEEEEEEEEEETTT--HHHHHHHHHHHHHH----HHHHHHHHHHHHHH-HHHHHHHHHHHHHHHS--HHHHHHHHHHHHHHHHHHHHT----EEEEEEEEEETTHHHHHHHHHHHTT--------EEEEEEEEEETEEEEEEEE--T------G-EEEEEETTEEEEEEEEETTTEEEE-HHHHHHHHHHHHHHTT-EEEEEE-GGHHHHHHHHHHHHHHT--EEEEEEEEEEEEHTTTT--HHHHHHHHHHHHHHTT---HHHHHHHHHHHHHH----EEHHHHHHHHHTS--HHHHHHHHHHHHHH\n",
            "--HHHHHHTT-EEEEEEEE-TTS-TT-HHHHHHHHHHTT--EEEE--TTTTGHHHHHHHHHHHHTT---GGGEEEEEHHHHHHHHHT-E-T--HHHHHHT---GEEEEEEE--TT--TTGGG-TT-TTS-EEE-THHHHHHHHHHHHHHHTT--EEEEEEE-TTHHHHHHHH-TT-EEE---S-EEE-TT--HHHHHHHHHHTT-EEEEE--TT-TTS-------GGGGG-HHHHHHHHHHTT-EEEEEEE-------EE--TT--HHHHHTT-EEEE----HHHHHHHHHHHTT-EEEEEEE----TT----\n",
            "--EEEEEE-TTTTSTTHHHHHHHHHHHHHT-EEEEEE---TTT-HHHHHHHHTT-HEEEEETTTEEEEEETT-T---GTTS--SHHHHHHHH-TT-EEEEEEETTTEEEHHTTTT-EEEEEEESSSHHHHHHHHHHHHT-----TSSHEEEEHHHHHHHHHHT--SEEEEEEE--HHHHHHHHHHS-SEEEEE--HHHHHHHHHSTSEEEEEE-----TT--T--HHHHHHHHHHHHHTS-HHHHHHHHHHHHT-HHHHHHH-TTHHHHHHHHHHHHT-------THHHHHHHTT--\n",
            "-EEEEEES--TEEEEE-TT--EEEEEEETEEEEEE-EEE-----T---EEEEEEEEEEEEEEEEEEEEEEEEEE-TTT--EEEEEEEEEEEEEEEET---EEEEEEEEEEEETEEEEEEEEEEEEEEEETTEEEEEEEEEEEEHTT---TTT--ETTEEEEEETTEEETEE-EEEEEEEEEEEE-S--TTEEEE-TT--EEEEEEETEEEEEE--GG-----T---EEEEEEEEEEEEEEEEEEEEEEEEEE-TTT--EEEEEEEEEEEEEEETT---EEEEEEEEEEEETEEEEEEEEEEEEEEEE-TEEEEEEEEEEEEGTT---TTT-EETEEEEEEETTEEETEE--EEE--\n",
            "--EEEEEEE--T-GGGHHHHHHHHHHHHHHHHHHHHT---T--EEEEEEESSS-HHHHHHHHHHHHHTT--EEEEEE--SSSHHHHHHHHHTS-EEEEETTT-HHHHHHT-HEEEEHTT--TTTSHHHHHHHHHH---SEEEEEE---TTHHHHHHHHHHHHHHTT-EEEEEE-TTT--H-HHHHHHHHHHTT-EEEEEEE--TT-HHHHHHHHHTT--EEEEETTT---H-HHHHHT---TEEEEE---GGHT-GGGGHHHHHHT--TT-TTS-EEEEEHHHHHHHHHHHHHHT--HHHHHHHHHHHTT-EEEEEEE---T----TT-EEEEEEEETTT-----\n",
            "--HHHHHHHHHHHHHH-HHHHHHHHHHHHHHHH-T----EEEEEETEEEEEEEEETTTT--HHHHHHHHHHHHHHH-\n",
            "---HHHHHHHHHHHHT--GGGHHGGGH--T-HHHHHHHHHHHHHHHHS----HHHHHHHHHHHHH-TT--\n",
            "---HHHHHHHHHHHHHHHHHTTHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH----HHHHHHHHHHHHHHHHHTTHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH-HHHHHHHHHHHHHHHHHTTHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH-HHHHHHHHHHHHHHHHHHHHTTHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH-\n",
            "--EEEEEEEEE--TTTTT--TT-----EEEEE--HHHHHTT-EEEEEEE-GGHHHHHHHHHHHHHT--T-SEEEEEE--S--GGG--HHHHHHHHHHH--TEEEEE--TTTT--HHHHHHHHHTT-TTEEEEEE-TT-HHHHHHHHHHHHHHHHHHHHH--\n",
            "----EEEEETT-EEEEEEEEEEEE--EEEEEEEEE-GHHHHHHHHHHHHHTT-EEEEEEETTTEEEEEEEEE-TTT--EEEEEEETTEEEEEEEEEEEET--EEEEEEEEE-GHHHHHHHHHHHHHHT-EEEEEEETTTEEEEEEEEE--\n",
            "-----HHHHTT-EEEEEEETTTEEEEEEETTEEEEEEEEEEEEEEE-GGGGGTT---TTSHHHHHHHHHTT-EEEEEE-----------EEEEEEE----TT--EEEEEEEEEEETTEEEEEEETTT-EEE----HHEE--TTT-EEEEEEEEEEEETTTEEEEEEEEEETTT-EEEEEEEE-TT-EEEEEETEEEEETS-EEEEEEEEEEESHEEHHEE-EEEEEEEEE-TTT---HHHEEHHHHHHHT--HHHHHHHH--EEEEEEEEEEE-----EEE-TTTEEEEEEEEEETTEEEEEEETTTTEEEEEEEEE----TT-EEEEEEEEEEE---TTEEEEEEEE-TTTT--EEEEEEEEEE-TTSEEEEEEEEETT---\n",
            "--HTT--HHHHHHHHHH----HHHHHHHHTS--E--TT-EEEETT--EEEEEEEETTTTEEEEEEEEES--T--EE-TT-------------SEEEE-EEEEEEEEGHHHHHHHHHHHHHT-TT-EEEEEEEHHHHHHHHHHHHHHHT--SEEEEEEES-GGG-HHHHHHHHHHHT----------EEEEE-TTT-TT-----TT-TTTTEEEEEE-TT-TTTEEEETTT-EEEEE-TT---H-THHHHHHHHHHHHHHHHHHHTT--HHHHHHHHHH----HHHHHHHHTS--EE-TT-EEEETT--EEEEEEEETTTTEEEEEEEEES--T--EE-T--------------SEEEE-EEEEEEEEGHHHHHHHHHHHHHT-TT-EEEEEEEHHHHHHHHHHHHHHHT--SEEEEEEESSGGGHHHHHHHHHHHHT----------EEEEE-TTS-TT---GGTT-TTTTEEEEEEETT--TTEEEEETT-EEEEEETT----T----EEEEEEETEEEE-\n",
            "----HHHHHHHHHHHHHHHHTT---S-EEEEHHHHHTTT---HHHHHHHHHTTT-----HHHHHHHHHHHTT--EEEEE--TTTT--------EEEEEEE-HHHHHHHHHHHHH-TT-EEEEEHHHHHHHHHHHHHHHT---EEEEEE--EEEEEE-TTT-EEEEEEE--TSHHHHHHHHH-HHHHH--EEEEEEE-HHHHHTTHHHHHHHHHHHHH--TTEEEEEETTEEEEEEE--TS--EEEEEE-T---TTTT-HHHHHHHHHT---EEE-ETT--EEE-HHHHHTTHHHHHHHHHHHH-T--ETTEEEEEEEEEGGGHHHHHHHHHHHHHHTT-EEEEEEEEEETTTEEE-TT---HHHHHHHHHHHHHTT-EEEEEEE--TGGTHHHHHHTT----EEEE---TT-TTTT---TT--HHHHHHHG---HHHHHHHHHHHHHHHHHHHHHHHHHT--EEE-TTHHHHHHHHTT---E----T---EEEEEEEETT-HHHHHHHHTT--HHHHT--HHGGGES--HHHHHHHHHHHHHHH--TT--EEEEEE-SSHHHHHHHHHHHHHHTT---S-EEEEEHHHTTTT---HHHHHHHHHTT---E--HHHHHHHHHHHTT--EEEEE--TTTT----G---EEEEEES-HHHHHHHHHHHHH-TT-EEEEESHHHHHHHHHHHHHHT----EEEE\n",
            "--EEEEEEE--TTTTGTTHHHHHHHHHHTT--EEEEEEE------HHHHHHHHHHHH----T-HHHHHHHTT-HHHHHHHHHTT-EEEEEEEEEEEEEETTHHHHHHHHHHHTTS--HHHHHHHHHTSEEEEETTTTEEHHHHHHHHHHHHHHHHHHHHHT------TT---HHHHHHHHHHHHT-\n",
            "--------TTTT--HHHHHHHHHHHHHHHHHHHHHHHHHTT---HHHHHHHHHHHHHHT---HHHHHHHHHHHHTT-HHHHHHHH-----EEEEEHTTHHHHHHHHHHTTT--HHHHHHHHHHH-HHHHHHHHHHHHHTT--HHHHHHHHHHHHHHHT--HHHHHHHHHHHHHHHHHHHHHHH------GG----TT--\n",
            "---GG-EEEEEE-TT--HHHTT-SEEEEEEEETTT-EEEEEEEESTTS--EEEEEEETTS-EEEEEEEEEETTEEEEEEEEEEEEETTEEEEEEE-\n",
            "-----HHHHHHHHHTS-TT-HHHHHHHHHTT---TTT--TT-EEEE--T-TTTTTTHHHHHHHHHHHHHHHHHTT-EEEEEEGGGSTSTTTHHHHHHHHHHHHHHHHT---EEEEEEEEE-------HHHHHHHHHHHHH-GGGEEEEEEE---SS----E--SEEEEEESSTTTTT-T--HHHHHH-------S-EHHHH-TTTTTHHHHHHHHHEEHHTT---HHHHHEEETT-TEEEEEEETTT-TT--TT---EEEEEEEEEEEEEE---TTT---TT---TTT-EEEEEE--TT-TTTT-----HHHHHHHHHHHHHTTEEEEEEEEE-TTS---TTT-HHHHHHHHHHHHTT--EEEEEE-TTTTTTTT---HHHHHHHHHHHHHT---SHHHHHHHE-TTEEEEEEEEETTS--TTT-T---EEEEEEE-TTTEETTGGTHHHHHHHHTT-----HHHHHHHHHHHH-TT----S-HHHHHHHHHHHT-\n",
            "--HHHHHHHHHHHHHHHHH-TT-EEEEE--SSTT-HHHHHHHHHHHT--EEEEE-TTTS---------EEEEEEEEEE-TTHHHHHHHHHHTT-EEEEEEE-S-GGGGGT--TTS--EEEEEHGGTT--THHHHHHTTT-EEEEETE-S-HHHHHHHHHHHHHHHHHHHHTTTTSS----HHHHHHHHHT-\n",
            "---HHTT---HHHHHHHHHHHHHHHHHHHHHHTT--HHHHHHHHHHHHHHHTTT--HHHHHHHHHHHHHHHHHHHH-TT--HHHHHHHHHHHHHHHHHHHHHHHH-\n",
            "--EEEEEEEEETTEEEEEEEETEEEEEEGGGGEEEEEEEEEGGGGTT--EEEEEEEEE-TTTEEEEEEEEE-S-EEEEEEEETT-EEEEEEEEEE-TTEEEEEEE-GG----------\n",
            "-------HHHHHHHHHTT---TT--HHHHHHHHHHTTT---TSSS-T-HTT----HHHHHHHHHHTS-EEEEEEHHH-S--HHHHHHTT--EEEEE-TTTS--------TTEEEEE--S-HHHHHHHHHHHHHHHHT-GEEEEEEETTTEEETTTTTEEEEE-HHHHHHEEEEEE-HHHH-HHHHHHHHHHHTTEEEEEEEE-TTTT--TEEEEEEEE-HTHEETTEHHHHHHHHHH-HHHHHHHHHHHHHHHHSTTT-EHHHHHHHHHHHHTEEEEEETTEEEEEEEEEEES--GHHHHHHHHHHHHT--HHHHHHH-TTTT---HH-HHHEHHHHHHHHHHTT-\n",
            "-EEEEEEEE-EEEEETTEEEEEETTTEEEEEEEEEEETEEEEEEEEESS--HHHHHHHHHHHHHHHHTT--EEEEE--HHHHHHHHTTTGGG-----TT--HEEEEEEEHHHHHTT-TTTTTTT-EEEEEEEEEEEEEEETTEEEEEEEEEEEEEE---SEEEEEEETEEEEEETTTEEEEEEEEEEETEEEEEEEEESS--HHHHHHHHHHHHHHHHTT--EEEEE--HHHHHHHHTTTGGG-----TT--HEEEEEEEHHHHHTT-TTTTTTT-EEEEEEEEEEEEEEETTEEEEEEEEEEEEEE---SEEEEEEETEEEEEETTTEEEEEEEEEEETEEEEEEEEESS--HHHHHHHHHHHHHHHHTT--EEEEE--HHHHHHHHTTTGGG-----TT--EEEEEEEEEHHHTTT-TTTTT----E-EEEEEEEEEEETTEEEEEEEEEEEEE-\n",
            "---EEEEEEEEEE--G--TT--TT-EEEEEEEE---TT-TTEEEEEEEEEEE-EE-TT-TTTTTHHEHHHHHHHTTT-EEEEEE--T--EEEEEEE-TTTSS-HHHHHHHHHHHHHTT-SEEEEE--STTHHHHHHHHHHHHHHTT-EEEEEEEEEEEE-----TT--TS--EEEEEEEEETTTT-HHEHHS-TT-EEEETT--EEEE-TT-EEEEEEEEEEE-THHHHHHHHHHHH-TT--HHHHHHHHHHHHHHHHHHHHTTTT-E-EEEE-EETTEEEEEEETT--GGEEEH-HHHHHHHT-EEEEEE--TT-TTTTT---HHHHHHHHT--HHHHHHHH-TT-EE--\n",
            "---TTEEEEEETTTEEEEEE-THHHHHHHHHHHHHHHHHHHHHH--EEEEEEEEEHHHHHHHHHHHHHHHHHHHT---TTEEEEEEEEEEETTEEEEEEEEEEETTTEEEEEEEEE-TT-TT----S-HHHHHHHHHHHHHHHHHHT---EEEEETTTEEEETTEEEEETTT-EEEEEEEEEEEEE--T-EEEEEEEEEEGGETTTEEE-GGGGGGTGEEEEEEEEETTE-TTTEEEEETTEEEEEEE-GHHHHHHTEEEEEEEEEETEEETTEEEEEEEETTEEEEEE----EE-GGGGEEEEEEEEEEEEEEEETTTT-EEEEEETT----EEEEEE-TT-EEEEEEEEEEEETTT-EEEEEETTHHTHHTTE-GG------H--EEEEEEEEETSEE-TTEEEEEEETTEEEEEE-THHHHHHHHHHHHHHHHHHHH---EEEEEEEEEEHHHEHE-HHHHHHHHHHT---TTEEEEEEEEEETTTEEEEEEEEEEETTTEEEEEHHE--TT-TTG----HHHHHHHHHHHHHHHHHHT---EEEEETTTEEEETTEEEEETTTTEEEEEEEE-EEEE---EEEEEEEEEETTTTEEEEEEGGGGTTTEEEEEEEEEETT--TTTEEEEETTEEEEEEE-GGHHHHHTTTEEEEEEEEEEE-TTEEEEEEEETTEEEE------EE-GGTTEEEEEE\n",
            "-----GGG---------TTT----EEEEEE-TTTEEE--T-T-HHH-TTEEEE-TTTTTTT-HHHH-TT----------TTT----EH-----TTTEE--TT-T----GTT-EEEEEE--T---\n",
            "--------THHHHHHHHTT--HHHHHHHHHH---T-E-TT-HHHHHHHHHHHHHTT-EEEE-TT-TGGHHHHHHHT---HHHHHHHHHHHHHHHHHHHHHTT-EEEEEEE---TTTTT--HHHHHHHHHHHHHH--TTEEEEEESSHHHHHHHHHHT-EEEEEEEEEEEE-TTS-E--EE-S-HHHHHHHHHHHHHHTTEEEEETEEE-----EEEEE-T-HHHHHHHHHHHHHHHHTT-EEEEEE---\n",
            "--EEEHHEHETS-TTHH-T--E---EEEEHHHHHHSTT--EEEEEEETTTEEEEEEETEEEEEETTEEEEE-TT-EEE--T---EEEEETT---EEEEEEEEE--\n",
            "-EEEEEE-T-HHHHHHHHHT-SEEEEE--TTTT--HHHHHHHHHHHTSEEEEEE-S--THHHHHHHHH-SEEEEEE-TTHHHHHHHHHTT-EEEEE-EE--T-TTHHHHHHHHHHHTTEEEEEEE---TTS-HHHHHHHHHHHHHHHHHHHHTT--EEEEEETT-EEEEEE-HHHHHHHHHET-----HHHHHHHSHHHHH-\n",
            "-EEEEEEEEE-----HHHHHHHHHHEHHHEE--TTTEEEEEEEEEEET---EEEEEEEEETTT-EEEEEEEEEEEEEEEEEE-TTS-EEEEEEEET-EEETTEEEEEEEEETTSHHHEEEEEEEE-TT-TTT--HHHHHHH-TTT-TT-EEETEE-S-HHHHHHHHHHHHHHEE--TT-EEEEEEEEEEETT-T--HHH-------HHHHHHHHHHHHHHHGGGG-----TT-HHHHHHHHHHHT--EEHHHHT----EETT----TT----TT---------TT--HHHHHHHHHT----TT--G---TTT--HHHHHHHH--TTHHHHHHHHHHHHHHH--HHHHHHHGGG-TTTHHHHHHHHHHHHHHHHHHH-HHHHHHHHHHHHHHHHTT--T---HHHHHHHHHHHHHHHHHHHHT----HHHHHHHHHHHHHHEEEEEEETTT-EEEEEEETTT-EEEEHH-TTT-EE--HHHHHHHHTH-----HHHHHHHHHHHHT--T-EEEE------TTG--EEEEEEEETEEETTEEEEE--TTEEEEEEEEE-TT--EHHHHHHSTT-HH-HHHHHEEE-EEEEEHTT------------TTTTTHHHHHHHHHHHHHHHHHTT-EEEEEETT--TT-TT-EEEEEETTTEEEEEEEETTEE-TTEEEEEETTEEEEEEE-GGG-T-S-EEEEEEE\n",
            "--EEEETTTT-TT--ETTHHHHHHHHHHHHHHHHHTT-SEEEEEE--TTTHHHHHHHHHHHHHHHHHTT--SEEEEEEE-HHHHT-HHHHHHHSTTTEEEEEEE-TTHHHHHHHHHH-TT-EEES----S--------------TT----TTEEE--------THHHHHHHHHHHHHHHHHHHS--SEEEEEE--HHHHHHHHHTT-EEEEEEE--------------HHHHHHHHHHHHHHHHT-EEEEEEEEE-TTTTTETTEEE-TT-EEETHHHHHHHHHHHHHHHHHTT-SEEEEEE--TTTHHHHHHHHHHHHHHHHHTT--SEEEEEEE-HHHHT-HHHHHHHSTTTEEEEEEE-TTHHHHHHHHHH-TT-EEES----S--------------TT----TTEEE--------THHHHHHHHHHHHHHHHHHHS--SEEEEEE--HHHHHHHHHTT-EEEEEEE--------------HHHHHHHHHHHHHHHHT-EEEEEEEEE-TTTTTETTEEE-TT-EEETHHHHHHHHHHHHHHHHHTT-SEEEEEE--TTTHHHHHHHHHHHHHHHHHTT--SEEEEEEEEHHHHT-HHHHHHHSTTTEEEEEEE-TTHHHHHHHHHH-TT-EEHHH-------G----------TTT----EEEEHHH----HTHHHHHHHHHHHHHHHHHHHS-T\n",
            "-EEEEEEEEEE---TT-EEEEEEEEEETT-TT-EE-TTS-EE-GGEEEEEEEEETTEEEEEE----------EEEEEEEEEEEEEEEEEEEEETSTTSEEEEEEEEEEEEEEEEE-----TT-EEEEEEEEE-TT-TT-EE-TTS-EE-GGEEEEEEEEETTEEEEEE----------EEEEEEEETEEEEEEEEEEEETSS-EEEEEEEE-\n",
            "----T--HHHHHHHHHHHHHHHHHHHHHHHTT----HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH----HHHHHHHHHHHHHHHHT-T--GGGGHHHHHHT-----HHHHHHHHHHHHHTT-----TT--HHHHHHHHHHHHHHHHHHHHHHHTT----HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH----HHHHHHHHHHHHHHHHTHTS-TTG-HHHHHTT-----HHHHHHHHHHHH-\n",
            "---GGS-EEEEETTTEEEEEE-TTGEEE-TTTT-HHHHHHEEEEEE----TTS--TEEEEEEETT--SEEEEEEE--HHHHHHHHHHHTT--HHHHHHHHTTTETT-EEEEEEEG-TTEEEEEEEEETTSEEEEEEEHHHHHTEEEEEEHHHHTT---EEEEEEEEEE--TTT-TTETEEE---TT-HHHHHHHHHHH-HHHHHHHHHHHHHHHHTHHHHTT----HHHHHHHHHHHHHHHHH-------\n",
            "-EEE--E-TTTEEEEEEETTTEEEEE-HHHHHHTT--GGG------HHH--TT--GG-GGGHHT----EEEEEETTS-EEEEEEEEEEE-TTS-EEEEEEEETTEEEE-HHTT---EEEEETTTEEEEE-HHHHHHHT--GGG------GGH--TT--GG-GGGHHTTT-EEEEEEETTS-EEEEEEEEEEE-TTS-EEEEEEEE-\n",
            "-T-TT-----E-TTSHHHHHHHHHHHH-TT-EEEEEEEETT-EGEEEEEEEEEEEEETEEEEEE----\n",
            "--EEE-TT-EEEHHHTT-EEEEEEEET-SS-EEEEEEEEEETTTEEE--TT--EEEEE----G-GG--EEEEEEEEEEE--TTEEEEEEEE-TSTTEEEEE---TT--EEEEEEEEEEE-T-EEEEEEETT-EEEEEHHHHHHHTTTT---EEEEEEEEEEEEETTEEEEEEEEEEEEEEEEEEEEEEHHHH-EEEEEEEEEEEETT-EEEEETTTEEEEEEEEE--TS-EEEEEEEEEETTTEEE--TT--EEEEE----G-GG--EEEEEEEEEEE--TTEEEEEEEE-TSTTEEEEE---TT--EEEEEEEEEEETT-EEEEEEETT-EEEE-HHHHHHHHTTT----EEEEEEEEEEETTTEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE--\n",
            "-EEEEEETTEEEEEGEEE-TT-TT--HHHHHTT----GTH-TEEEEEEEHHHHHHHHHHE----HHHHHHEETEE--\n",
            "--EEEEEES------THHHHHHHHHHHTT--EEEEEEEE-TTTEEEEEE-TTSTTS-EEEEEEEEEE---TT-HHHHHHHHHHHH-TT-EEEEE---STT--HHHHHHHHHHTTT-EEEEEE-S-HHHHHHHHHH-TT--EEEEESSHHHHHHHHHT-EEEETTT----HHHHHHHHHTT-EEEEEES-SHHHHHHHHHTT--EEEEE--TEE-----\n",
            "---HHHHHHHHHHHHHHHHHHEHH--TT---EE-HHHHHHHHHHHHHHHHHHHHTT-EEEEEEEEEEETTEEEEEEEETTTS--EEETT--EEEEEEEEEETT-HHHHEEEHHHHHHHETT-TTEEEEETTEEEEEEEESSS-EEEEEEEETT-EEEEHHHH-HHHHHHHHHHHHHHTT-HHHHEEEGG-S-----EEEEEEEEHTTEEEEEETTTEEEEEEEEEEEEEEEEE--HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHEEHHE-TT------HHHHHHHHHHHHHHHHHHHHTT-EEEEEEEEEEETTEEEEEEEETTTS-EEEETTEEEEEEEEEEEETT--HHHEEEHHHHHHHETTSTTEEEEETTEEEEEEEESSS-EEEEEEEETT-EEEEHHH--SHHHHHHHHHHHHHTT-HSEEEEEGG-S-GG---EEEEEEEETTEEEEEETT-EEEEEEEEEEEEEEEEES-HHHHHHHHHHHH-\n",
            "---EEEEEEE-TTSHHHHHHHHHHHTTT-EEEEESSEE--THHHHHTT--E----HHHHHHHHHHHHHHTT-EEEEEETT-TTTT-HHHHHHHHHHHTT--EEEEE----HHHHHHHTT----HHHHHHHHHHHHT--TTS---T-SEEEEE--HHHHHHHHS-TT-HHHHHHHTT--GGEEEEEEEGG-TT--T----HHHHHHTHHHHHHEEH--TTTT-EEEEEEEEE-TTSHHHHHHHHHHHTT--EEEEEES-E-TTHHHHHTT----HHHHHHHHHHHHHHTT-EEEEEE-T-TTTTTHHHHHHHHHHHTT--EEEEE----HHHHHHHTT----HHHHHHHHHHHHT--TTS---T-SEEEEEE-T-TTTHHHHHHHHHS-TT-HHHHHHHTT-TT-EEEEEEEEE-TT--TT----HHHEETEEEEEEEEH---\n",
            "--EEEEEEE--TT-HHHHHHHHHHHTT----EEEEE-TTHHHHHHHHHTTTS-EEEEE--HTTTHHHHHHHHHTT--EEEEEETTTT-EEEEEEEETTTTS-EEEEEES-TTHHHHHHHHTT--EEEEEE-SSHHHHHHTT--TEEEEEEEES-EEEETTEEEEEEEESSEEEEEEEGGGHHHHIIHHHHHHHHHHHHHT-\n",
            "-EEEEEEETTS---SEEEEEEEEEEETTEEEEHHEEEHHHHHTTT---EEEEEEEEG--HHHH-S-EEEEEEEETTEEEEEEEEEE-TTEEEEEETEEEEEEETTEEEEEEEETTEEEEEEEE-EETTT--EEEEEEEEE----T---HHHHHHHTTT----GGG-EEEEETTSHHEHHHHEEHHHHEEHHTT---HHHHHHHHHEEEEE-ETHHHHETTEEEETT-EEEEEETTEEEEEEEET--HHHHHHHHHHHHHHHHHHHHHHTT-HHHHHHHHHHH-TT--HHHHHHHHHHHHHHHHH-HHHHHHHHHHHHHEE-EE--TT------TTEEEHHHHHHHT---HHHHHHHHHT-SEEEEEEETTEEEEE-TTTTTTTEEEEEEEE-TTS-HHHHHHHHHHHHHHHHHHEEEEEEEETT------EEEEEEEEEETT-EE-HHHHHHHHHHHTT---EEEEEEEEG--HHHHTT-EEEEEEEETTEEEEEEEEEE-TT-EEEEETEEEEEE-TTEEEEEEEETTEEEEEEHHHHHTT---EEEEEEEE-----T---HHHHHHHHTT----GGG--EEE--HHHHHHHHHHHHHHHHHHHTT---HHHHHHHHHET-----HHHHHETTEEEETT-EEEEEETTEEEEEEESS--HHHHHHHHHHHHHHHHHHHHHHTT-HHHHHHHHHHH-TTT\n",
            "---EEEEEEEHHHHHTTTT--EEEEEEEEEEETTTEEEEEEEE-GGG--EEEEEETT-EEEEEHTTT-EEEEEEEETTEEEEEEEEEEEEEEEEETTEEEEEEEEEE-T----EEEEEEEETTTEEEEEEEE-HHHHHHHHHEEEE-HHHHHHT-----EEEEEEEEEEETTTEEEEEEE--GGG---EEEEE-T-EEEEEETTTTEEEEEEEETTTEEEEEEEEEEEEEEEETTEEEEEEEEE--T----EEEEEEEETTTEEEEEEEEEEEE--\n",
            "-EEEEEE-TT--HHHHHHHHHHHT-EEEEEEEEETTT-EETT---------TGGTTEEEEEEES-TT-HHHHHHHHHHT-EEEEE------EEEEEE--STTHHHHHHHHHHHHHHHHHH--SEEEEEEEE---HHHHHHHHHHHHHTT--EEEEE---HHHHHTSHHHHHHHHHHHHH--EEEEEE---HHHHHHHHHHTT-SEEEEEEGGGT-HHHHHHHHHHHHHT---S--HHHHHHHHHHHHHHHHH-HHHHHHHHHHHHHHHTS-HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH-\n",
            "--EEEEEEE-TTTTTHHHHHHHHHHHHHHHHTT-EEEEEE--GGHHHHHHHHHT----HHHHHHHHHHHHHTT--HHHHHHHHTT-SEEEE-GGGGGHHTTTT--ETTT---HHHHHHHHHHHHHHHT--EEEEEE-----GGGTTS--EEEEESS--HHHHHHHTT-EEEEE-EEEEEE-TTTEEEEEE---HHHHHHHHHHHHHHHHHHHHHH-T-TEEEEEEEE-----HHHHHHHHHHHHHHHHTT-EEEEE-S-GHHHHHHHHHHT----H-HHHHHHHHHH-TT--HHHHHHHHTT-SEEEE-GGGGGHHTTTT--ETTT---HHHHHHHHHHHHHHTT--EEEEEE-----GGGTTS--EEEEESS--HHHHHHHTT-EEEEE-EEEEEE-TTTEEEEEE----THHHHHHHHHHHHHHHHHHH--\n",
            "--EEEEEEE-TTTTHHHHHHHHHHHTT--EEEESHHHHTHS-TT-HHHHHHHHHHT-EEEETTS-E-HHHHHHHH---HHHHHIIIIIIHHHHHHHHHHHHHH--S-SEEEEEE---TTT------EEEEEEE-HHHHHHHHHHH----HHHHHHHHHHHH-HHHHHHHHHHHHHTT--HHHHHHHHHHHHHHHHHHHHHHHHHH--TT-EEEEEEE-STTTSHHHHHHHHHHTT--EEEESHHTTT-S-TT-HHHHHHHHHTT-EEEETTS-EEHHHHHHHH---HHHHHIIIIIIHHHHHHHHHHHHHH----SEEEEEEE--TTTE-----EEEEEEE-HHHHHHHHHH-----HHHHHHHHHHHH-HHHHHHHHHHHHH-T--HHHHHHHHHHHHHHHHHHHHHHHHHH--TT-EEEEEEE-STTSHHHHHHHHHHHTT--EEEHHHHHHT---TTHHHHHHHHHHHHHEE--S-----II-HHHHHHHHHHHHHHT--TSEEEEEEEEETTTE--E--EEEEEEE-HHHHHHHH-------HHHHHHHHHHHHHHHHHHHEEEEEEE-T--HHHHHHHHHHHHHHHHHHHHHHHHH----\n",
            "---HHHHHHHHHHHHHTT--HHHHHHHHHHTT--TTEEEEEETTHHHHHHHHHHHHHHHHHHHHHHHHHH---HHHHHHHHHHHHHHHHHT-HHHHHHHHHHHHTT-HHHHHHHHHHHHHHHHHHHHHHHHT--TT---HHHHHHHHHHHHHHHHHHHHHHHHHTT---HHHHHHHHHHHHHHHHTTT-TGGHHHHHHHHHHHHHTT--HHHHHHHHHHTT--THEEEHHHTTHHHHHHHHHHHHHHHHHHHHHHHHHH----HHHHHHHHHHHHHHHHT-HHHHHHHHHHHHTT-HHHHHHHHHHHHHHHHHHHHHHHTT--TT---HHHHHHHHHHHHHHHHHHHHHHHHHTTT-HHHHHHHHHHHHHHHHHHH-\n",
            "-EEEET-EEEEEEEEE-----TTEEEEEEEETSSHHHHHHHHHHHHHTT--EEEEESSTTHHHHHH-SEEEEEES--HHHHHHHHHHHGT---EEEEEEEHH-HHHHHHHHHHTT--HHHHHHHHHHHHHH-HHHHHHHHHHHHHHHHT--T---HHHHHHHHHHHHH--EEEEEE-TTHHHHHHHHHHHHHHTT-EEEEE---------TEEEEEEEETTSSHHHHHHHHHHHHHTT-EEEEEES---THHHHH-SEEEEEES--HHHHHHHHHHHTT---EEEEEEEHH-HHHHHHHHHHTT--HHHHHHHHHHHHHTGGHHHHHHHHHHHHHHHHT--T---HHHHHHHHHHHHH--EEEEEE-TTHHHHHHHHHHHHHHTT-EEEEE---------TEEEEEEEETTSSHHHHHHHHHHHHHTT-EEEEEES---THHHHH-SEEEEEES--HHHHHHHHHHH-T---EEEEEEHHHHHHHHHHHHHHTT--HHHHHHHHHHHHHTGGHHHHHHHHHHHHHHHHT--T---HHHHHHHHHHHHH--EEEEEE-TTHHHHHHHHHHHHHHTT-EEEEE---------TEEEEEEEETTSSHHHHHHHHHHHHHTT-EEEEEESE--THHHHH-SEEEEEES--HHHHHHHHHHH-T---EEEEEEEEEEEHHHHHHHHHTT--HHHHHHHHHHHHHH\n",
            "---HHHHHHHHHHHHHHHHHHHHHHHHTT-EEEEEEEE--TT--T-EEEEEEETEEEEEEEEEEEEEETTT-TT-EEEETEEEEEEETT-TTS--S--EEEEEEEEHTT---EEEEE--T---TT----T-HHHHHHHHHHHHGGTT-EEEEEETTE-HHHEEEEEETTTTEEEEEEE---TTS-HHHHHHHHHHHTTT-EEEEEEEEETTTT----HHHEHHEE--TT--EEEEEEEEETTEEEEEEESEEEEEEEEE-TT-EEEEE----TT---EEEEEEEEEEHHHHH----HHHHHHHHHHHHHHHHHHHHHHHHTT-EEEEEEEE--TT--T-EEEEEEETEEEEEEEEEEEEEETTT-EEETEEEEETEEEEEEETT-TTS--S--EEEEEEEEHTT---EEEEE------TTT---T-HHHHHHHHHHHHGGGT-EEEEEETTT-HHHTEEEE-TTTTTEEEEEE--GTGS-HHHHHHHHHHHTTT-EEEEEEEEETTTT----HHEEEEEE--TT--EEEEEEEETTTEEEEEETSEEEEEEEEE-TT-EEEEEEE--TT-HHHHHHHIHHHHHH--\n",
            "--GG---EEEE-TTEEEE-HHHHHHHHHHHHHHHHHHH-TTT---------HHHHHHHHHHHHHHHHHHHTT-----HHHHHHHHHHHHHHHHII-TTTHHHHHHHHHHHHHHEEEEEE----TT-EEEEEE-------T-EEEEEEEETTEEEEEEEESTHHHHHTTHHTT--HH--HHEEHHHHHHHS-TGGTEEEEEEEEEEEETT--EEEEEEEEEEE--GGGG--HHHHHHHHHHHHHHHHHHHHHHH----HHHHHHHHHHH-HHHHHHHHHHHHHHS-TT--TT--TTGEEEHHHHHHHTTT--HHHHHHHHHHHHHHTTTEEEE-TTTEEEE-HHHHHHHHT--HHHHHHHHHHTTTT-\n",
            "----EEEEEEE-GGT--TTHHHHHHHHHHHHHHHH----EEEEEEEEE-TT-S-EEEEE---THHHHHHHTHTTHHHHHHHHHHTT-EEEEE----TT-HHHHHHHHHHTT-EEEEEEHHHHHHHHT---EEEE-T-----TT-HHHHHHHHHHHHHHHHHTT-EEEEEE--HHHHHHHHTTT-SEEEEE----HHHHHHHHHTTSEEEEE--T-------S-EEEEGGGG-\n",
            "--HHHHHHHHHHHHHTT---GGGGTT--G----TT---HHHHHHHHHHHHHH---HHHHHHHHHHHHHHT--HHHHHHHHHHHHHHH----HHHHHHHHHHHHHHHHHEHTT--E-TTHHHTHHTTT-HHHHH-TT----HHHHHHHHHHHHHH--HHHHHHHHHHHHHHT--HHHHHHHHHHHHHHH---HHHHHHHHHTHHHHHHHHEETT-EEETT-H-TTTTTT--EE---TT---HHHHHHHHHHHHHHH--HHHHHHHHHHHHHHT--HHHHHHHHHHHHHHH----HHHHHHHHHHHHHHHHHHHTT----TTS-EE----EE--GG---TT---HHHHHHHHHHHHHHH--HHHHHHHHHHHHHHT--HHHHHHHHHHHHHHH--THHHHHHHHHHHHHHHHHHHHTT----HHHHHHHHHHHHHHHHTT---GGGG---------TT---HHHHHHHHHHHHHHS--HHHHHHHHHHHHHHT--HHHHHHHHHHHHHHT----HHHHHHHHTHHHHHHHH-ETT-EEEEEEEE-TT-EEEEEEEEE-TT-----HHHHHHHHHHHHH--HHHHHHHHHHHHHHT--HHHHHHHHHHHHHHHHTHHHHHHHHHHHHHHHHHHHHHT-\n",
            "--EEEEEEEEE-GGGG---TTEEEEHHHHHHHHHHHHHHHHHTT-EEEEEE-TTTTTTTHHHHHHHHHHTT-EEEEEE-GGGHHHHHHTT--EEEEEE-S-HHHHHHHHHHHHHHT--EEEE-TGG--HHHHTT-TT-EEEEEEE---HHHHHHHHHH--EE---HHHHHEEEETTEEEEE--HHTTT---TTEEHHHHTHHHHHHHHHHHHHHHT-EEEEEE--TTTTTTHHHHHHHHHHTT-EEEEEE-GGGHHHHHHTT--EEEEEEES-HHHHHHHHHHHHHHT--EEEE-TGG--HHHHTT-TT-EEEEEEE---HHHHHHHHTT--EEE---HEEEEEEEEEEEEEEEE---TTT---TTEEEEE-TTHHHHHHHHHHHHHHT-EEEEEE--TTTTTTHHHHHHHHHHTT-EEEEEE-GGGHHHHHHTT--EEEEEEES-HHHHHHHHHHHHHHT--EEEE-TTG--HHHHTT-TT-EEEEEEE---HHHHHHHHTT--EEE---HHEEEEEEEEEEEEEEE---TTT---TTEEEEE-TTHHHHHHHHHHHHHHT-EEEEEE--TTTTTTHHHHHHHHHHTT-EEEEEE-GGGHHHHHHTT--EEEEEEES-HHHHHHHHHHHHHHT--EEEE-TGG--HHHHTT-TTEEEEEEEEE--HHHHHHHHHH--EE----GHHHH-\n",
            "----HHHHHHHHHHTT--EEE------E---T---GGG---T--ETTEEEE-TTTEEEEEEEETTEEEEHHHHHHHHTTTT--EES--HHHHHHTT--TTEEEEEEEEETTTTEEEEEE-TTEEEEEEETT---TTT--EEE-HHHHHHHHHHHT---EEEEEETTT---HHHHHHHHHHTT--EEE-----EEEE-----HGGH--T--EETEEEEETTTEEEEEEEETTEEEEHHHHHHHTTTT--EEES--HHHHHHTT--TTEEEEEEEEETTTTEEEEEEETTEEEEEEETTE---TT-EEEEEHHHHHHHHHHTT---EEEEEE-\n",
            "------HHHHHHHHHHH--HT-T-EEEEEEEETTTEEEEEEEEEEEE-HHHHHH---T---EEEEEEEEEEETTTEEEEEEEEEEEE-TT--HHHHHHHHHHHHHHHHTT--EEEEEEEE--HHHHHHHHHHHHT--\n",
            "---HHHHHHHHHHHHHHHTT--TT-EEEEEEETTTTEEEEEEEEEEEEE-TT---EEEEEEEETTTTEEEEEEEEEEEEEEEEEEEET----HHHHHHHHHHHHHHHHHHHHHHHT-EEEEEEEEEEEEEEEEEEHHHHHHHHHHHHHHHHHHTT--TT-EEEEEEETTTTEEEEEEEEEEEES------EEEEEEEETTTTEEEEEEEEEEEEEEEEEEEEE----HHHHHHHHHHHHHHHHHHHHHHHTEEEEEE-EEEEEEEEEEE-\n",
            "-----------------EEEETTTHHHHHHHTT-HT---TTEEEEEEEEE-TTTEEEEEEEETTTEEEEEEEEEEETTEEEEEEEEEEEE-----EEEEEEEEEEEEETTTEEEEEEEETHHHHHHHHHHHHHS-HHHHEEEETTTEEEEEEE-TTTEEEEEEETTTTTTTTTTTTT--EE----EEEEEEEHTT--EEETTTTT-----HHHHHHHTT-EEETTTTTT-EEEEEEEETTT-EEEEEEEEEEGGGS--TT-EEEEEE-TT--EESEEEEEEEETTT-EEEEEEEETT---TEEEEEETTSSEEEEEEETT---\n",
            "--EEEEEEEEE-TT--HHHHHHHHHHHHHHHHHHHHH-H-HHH-----EEEEEEEEEETTEEEEEEEEETTEEHHHHHHHHH---T---HHHHEHH-HHHHHH----\n",
            "--HHHHHHHHHHHHHH--H--HHHHHHHHHHHHHHHHHHHHHHHHTT---HHHHHHHHHHHHHHHHHHTT--HHHHHHHHT--\n",
            "-EEEEE-----GHE-HHHHHHHHHHHHT--TTEEEEEETT--EEEEEETTT-TTHHHHHHHHHHHHST--T--EEEEE--TEEHHHHHT---EEEEEETTEEEET-ETEEEEEEEETTEEEEEEEEE-\n",
            "---EEEEEE--TTTT-HHHHHHHHHHHHHHTT--EEEEEEEST-HHHHHHHHHTT-SEEEEEE-SS---TTTT---HHHHHHHHHTT-EEEEEEEEEET--HHHHTTHTT--HHHHHHHHHHHHHGTT-EEEEEEEEEE-TTTEEEHHHHHHTT-EEEETEEEEEE-GGEEEEEE--EEEEEEEEE----\n",
            "-------T--EEEEEEEE-TT---EEEEEEEEEEEEEETHHH-EEEEEEEEEEEEEEEEEEETTT---TT--EEEEEEEEEEEE--EEEEEEEEGTTEETEEEEEEEE-EEEEEEE-TTS-EE---------HHHHHHHHH-H--HHHHHHHHHHHTS---TTEEEEEEEEE-TT--EEEEEEEEEEEEEEETTT--EEEEEEEEEEEEEEEEEEETTT---TT--EEEEEEEEEEEEEEEEEEEEEEGTTEETEEEEEEEE-EEEEEEEETTS-EEE--------HHHHHHHH-----GG-----EEEEEEE--TTEEEEEEEEE-TT--EEEEEEEEEEEEEEETTT--EEEEEEEEEEEEEEEEEEETTTE-ETT-EEEEEEEEEEEEEEEEEEEEEEETTTEEEEEEEEEET-EEEEEEE-TTS-EEE--------HHHHHHHHH-HHHHHHHH--\n",
            "-TT--TT---HTHH--HHHHHHHHHHHHTT-EE----TTT-----ETTTEE-TTT-HHHHHHHHTT---TTT--T--T---TTHHHHHHHHHHHHTT--TT-EEEEEEE--------HHHHHHHHT--HT--EEE---EEEE--TTE--TTE--TT-TTTTEEEEEESHHHHHHHHHHHHT---HHHHHHHHTTTT-EEEEEEEEEEETT--EEEEEE-GGG-H-------TT-EEEEHHHTTTTEEEEEEE-TT-TEEEEEEEETTT-EEEEEEEE---STTEEEEEGGHTT-EEEEEEEEEEES-TTT--EEEEEE-E---TT-------TS--------G--------\n",
            "--EEEETS-EEEEEE-TTT----HHHHHHHHTTS-THHHHHHHHHHHHHHHHHHHTT--TT-EEEEEE--TT--HHHHHHHTTTTEEEEEEEEETT-E--TTTTT-TEEEEEESS--HHHHHHHHHHH--SEEEEEE-TT-HHHHHHHHHH--TTEEEEEE---GGGG-------HHHHHHHT-TT-------------HHHHHHHHHH-GGGG-----GGGHHHHHHHHHHHHHHHTTTEEEEE--TT----EEHEEEEEEHHH--TTSTT-HHHHHHHHHHHHHHTT--\n",
            "---EEEEEEEEEEEEEETEEEEEEEEEETTSTTEEEEEEEEE-TTTT-EEEEEEEEEEEEEHEETTTEEEE--HHHHHHHHHHHHHHHTT-EEEEETTEEEEEETTS-EEEESS--HHHHHTT-EEEEEEEEEEEEEEEEEEEEEEEES---EEEEEEEEEETTEEEEEEEEEEE-TT-EETEEEEEEEE--TT--EEETTTEEEEETTSEEEEEEEETT--HHHHHHHHHHTT--HHHHHHHHHTT----T-----HHHHHHHHHHHHHHHHTT-------------T---EEEEEEEEEEETHHHHHTEEET--TTEEETTHHHHHHHHTT---TTTEEEEEEETTTEEEEEEEEEE-TTT-EEEEEEEEEEEEHHHHHHHHHHTTTEE--TTT--EEEEEE---TTHHHHHHT--HHHHHHHHHH----GEEEEEEEE-TT-HHHHHHHHHTTT---TTGG--EEEETTS-EE-TTTT--EEEEEEEETTEE-HHHHHHHHHHHHHHHHHHHHT-HHHHHHHHHTTT-----TTST----EEETTEEEEEETTTTTSEEEE-HHHHHHHHHHHHTT------EEEEETTT-EEEEEE--HHHHHHHHHHHHHHHTT--HHHHHHHHHHHHT-EEEEEETT-EEEEEEEEETT-TTEEEEEEEE--TTTT--HHHHHHHHHHHHHHHHTT-EEEEEET\n",
            "------------GG---TT--HHHHHHHHHTT-EEEEE-THHHHHHHHHHHHHHHHHHHEEE-TTT-HHHHHHHHHHH-TTGGGGHHHHHHHHHHHTT------------------------GGG--TTT--HHHHHHHHHTT-EEEEE-TTHHHHHHHHHHSGGGGHHEEEE--TT-HHHHHHHHHHH-TTGGGGHHHHHHHHHHHTT--EE----------------HH-GGGG-TTT--HHHHHHHHHTT-EEEEE-TTHHHHHHHHHHGGGGGHHEEEE--TT-HHHHHHHHHHH-TTGGGGHHHHHHHHHHHTT--EE---------------HHHGGGGG-TTT--HHHHHHHHHTT-EEEEE-TTHHHHHHHHHHGGGGGHHEEEE--TT-HHHHHHHHHHH-TTGGGGHHHHHHHHHHHTT---E---------------HHHGGGGG-TTT--HHHHHHHHHTT-EEEEE-TTHHHHHHHHHH-GGGGHHHHHGGGTT-HHHHHHHHHHH-TGGGGG-HHHHHHHHHHTT------EEEE--\n",
            "-HHHHHHHHEEET--HHHHHHHHHHHHHHHHHHHHTT--EEEEEEEEEETTTEEEEEEEEEEEEEEEEETTTTEEEEEEEEE-----HHHHHHHHHHHHTTSEEEEEEEEEEEEEEEEEEEHHHHHHHHHHHHT--HHHHHHHHHHHHHHHHHHHHTT--EEEEEEEEEETTTEEEEEEEEEEEEEEEEETTTTEEEEEEEEEES---THHHHHHHHHHHTTSEEEEEEEEEEEE--\n",
            "----------EEEEEEEEEEEEE-S-EEGGGGEEE-ETT-EEEEEEEEEEEETTS-EEEEEEETTGEEEEEEEETTEEEEEEGGE-TTEEEEEEEETTEEEEEEEETTEEEEEETTTTEHHHHHHHHHHHHHHH-\n",
            "-------------\n",
            "HHHHHHHHHHHTTT-EEEEE-TT-TT--TEEEETEEEEEEEEE--TT---EHHHTS--HHHHHHHHHETT--------HHHHHHHT-EEEEEEEEEETT-H---HHHHHHHHHHHHH-TT---EE-TTTEEEE-\n",
            "----EEEEEHHEETSHHHHHHHHHHHHHHHHH-TTG-EEEEEEEEEE-TTEEEETTEEEEEEETTEEEEEEEEEEEES-G---GGGEEEETT-EEEEEEE---TT-EEEEEEEETTTSHHHHHHHHHHHHHH--TTEEEEEE-TT-------HHHHHHHHHHHHHHHHHHHHHHHHHHHHHH-TEEEEEEEEEE---EEEE--EEE-HHHHHHH--HHHHHHHHHTT-HHHHHHHHHH-EEEEEE---HHHHHHHHHHHHHHHHH-TTGGEEEEEEEEEE-HHHEEETTEEEEEEETTEEEEEEEEEEE-S-G---GGGEEEETT-EEEEEEE---T-EEEEEEEEETTTSHHHHHHHHHHHHHH--TTEEEEEE-TT-------HHHHHHHHHHHHHHHHHHHHHHHHHHHHHH--TEEEEEEEEE-T-EEEE--EHHHHHHHHTT---HHEHHHH----GEHHHHH-\n",
            "-----T--HHHHHHHHHHHHHHHHHTTT---ET-EEEEEEEHHHHHHHHHHHHHHHTTSTTEEEEEETTEEEEEEEEEEEEEETTTS--EEEEEEHHHHHHHHHHHHHHH-\n",
            "--------------GG--------HHHHHHHHT--GGGHHT---EE-TT--HGHHHHHHHHHHHHHE-TTHHHHH--HHHHHHHHHHHHHHHHT---HHHHHHH--HHHHHHHHHHHHHHHHHHHHHT-\n",
            "--------EEEEEE-TTEEEEEEEEEEE---TT----HHHHHHHHHHHHHTHHEHHHHHHHHHHHTTTTS-T---HHHHHHHHHHHHT---HTT-HEEEEEEEEEEEEEEE-TT----EEHHHH-HTTEEEEE-TTT-EEEEEE---T---GGGHHHHHHHHHHHHHHHHHHT-EEEEEE-TT-HHHHHHH-TTTTT----T---HHHHHHHHHHHHHHHHHHHHTTT--EEEEEEE----TTT-TTT-HHHHHHHHHHHHTT-EEEEEE---TTTT--H-HHHHHHHHHT-SEEEEEES---HHHHHHHHHTT--SEEEE-GGGT--HHHHHHHHHHHTH-TT----------TT-------\n",
            "----HHHHHHHHHHHHHHHHHHHHT-HHHHHHHHHHHHSS--THHHHHHHHHHHHHHHHHHHHHHHT-HHHHHHHHHHHTTT-THHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH-----\n",
            "---HEEEEEEEEEEEE-HTS---TEEEEEEEEEEEEE---EEEHHHHHTTTS-TT---EEEEEEEEHHT--HHHHHHGTHHHHHHHHHHHHHHHHHHT----EEEEETSEEEHHHHHHHHHHHHHH-S-TTHHHEEEEHHETEEEEEEETTT-EEEEEEEHTT--TTTEEEEEEEETTGHHHHHHHHHHTT-EEEEEEE-\n",
            "-EEEEEEEEEEEEEEEE-EEEEETEEEEEEETSEEEEEEEETTEEEEEEEEEEEESTSEEEEEEEETTEEEEEEEETTS--EEEEEEEEEEEETTEEEEEEEEEESS-EEEEEEEEEEE-\n",
            "-EEEEEEEEEEEEEEEEEEEEEEEEEETEEEEEEEETT-EEEEEEEEE-TTS-EEEEEETTTTEEEEEEESSS--EEEEEEEEE-TT--EEEEEETTEEEEEEE------EEEEEEEES-T----EEEEEEEEEEETTEEEEEETTS---H------TTEEEEEEEEEEET--TTEEEEEEEEEEEE-TTS-EEEEEEEE--S-EEEEEEEEEEEEEEEEETTTTEEEEEEEEE--TTTTTT-EEEEE-TTTEEEEEEETTTTEEEEEEEETTT-EEEEEEEEETTTEEEEEEE-----------EEEEEEEETSEEEEEEETS--EEEEEEEEETTT-EEEEEEEEEE-EEEEETTEEEEEEEEEEEEEEEETEEEEEEETTEEEEEEEEEE-TTT-EEEE-EEEEEEEEEEEEEEEEEEETEEEEEEEEEEEETT-HHHHHHHHHTT-EEEEETTEEEEEEEEEE--STTTEEEEEEEEEE-S-TT-EEEEEEEEETTEEEEEEEEEETTSS-EEEEESSSEE-EEEEES-HHEE----HHHEEE--TT-EEEEEEEE-SEEEEEEEEEEGGEEEEETSEEEEEEEETTEEEEEEEETTT-EEEEEEEEEETTTEEEEEEEEET--TT---S--EEEEEEEE-TT--EEEEEEE-TT---EEEEEEEETTTEEEEEEEETTEEEEEEEET--------\n",
            "--HHHHHHHHHTT-EEEEEEE-HHHHHHHHHHHHT--THHHHHHTTEEEEE-TTTTTTTHHHHHHHHHHHH--EEEEEETTTHHHTT---HHHHH-TT-TT-EEEEEE-TT-EEEEE-TTS-HHHHHHHHHHHHHHHHHH-TT-EEEEEETT-HHHHHHHHTTT--EEEEEE-HHHHHHHHHHHHT--THHHHHHTTEEEEE-TTTTTTSHHHHHHHHHHHH--EEEEEETT-HHHTT---HHHHH-HHHHT-EEEHHH-TT-EEEEE-TTS-HHHHHHHHHHHHHHHHT--HHHHHHHHHHHHHHHHTT--EEEEE--HHHHHHHHHHHHT--THHHHHHTTEEEEE-TT-TTTSHHHHHHHHHHHH--EEEEEETT-HHHTT---HHHHH-HHHHT-EEEEEE-TT-EEEEE-TTS-HHHHHHHHHHHHHHH---EHHHHHTTTT-EEEEEE--HHHHHHHHHHHHT--THHHHHHTTEEEEE--T-TTTSHHHHHHHHHHHH--EEEEEETT-HHHTT---THHHH-THHTT-EEEEEE-TT-EEEEEETTS-HHHHHHHHHHHHHHHH-\n",
            "-HHHHHHHH----HHHHHHHHHHHTT--HHHHHHHHHHHHHHHHT--TT-EEEEEE-TTGGEEEES-GGGH-T--EEEEEEEEE--HHHHHHHHHHT-EEEEEE---------TT-TT----TT--EEEEEETT-ETT---T--TTTT---HHHTT-HHHH-S-EEEETTEEEEEEEEEEESTTEEEEEEEEHHHTT--GG-HHHHHHHHHH--GGG--S---EHHHHHHHHHHHHHHHHTT-EEHHHHHHHHHHHHHHHH-TTT-EEEEEEETT---EEEEEEEET-HHHHHHHHHHHHHTT-EEEEEEEEETT-EEEEETEEEHHHHHHHHHHHHHHHHHHHHHHHTTT--TT----HHHHHHHHHHHT-----EEEEEEETT---HHHHHHHHHHHHHHHHT--TT-EEEEEE-T-GGEEEES-GGGH-T--EEEEEEEEE--HHHHHHHHHHT-EEEEEE---------TT-TT----TT--EEEEEE-T-TTTGGGTT-TTTT--HHHHHHHHHHHTS--EEEETEEEEEEEEE---TTEEEEEEEEHHHHHHHHHHHHHHHHT--GGGGTT---EEHHHHHHHHHHHHHHHTT-HHHHHHHHHHHHHIHHHH-TTTEEEEEEE-TT---EEEEEEEEE-HHHHHHHHHHHHHTT-EEEEEEEEETT-EEEEETEEEHHHHHHHHHHHHHHHHT-\n",
            "--EEEEEEEEETHEETTEEEEEEEEE-HHHHHHHHHHHHHHHHT-----TT---TTT-----HHHHHHTTTS-EEE--TTTHHHHHHHHHHHHHT-HHHHHHHHHHHH---TTS--HHHHHHHHHH-HHHHHHHHHH-TT--TTEEEEEEEETTTT-----EE---HHHHHHHHHHHHHT-HHHHHHHHHHHHHHHHHHHHHHHHHTTTETTTEEEEEEHHHEETTEEEEEE-EEEE--TT-EEE----GGTT---TTEE--HHHHHHHHHHHHTT--EEEEHHHHHH--EEEEEEEE-HHHHHHHHHHHHHHHHT-----T----TTT-----HHHHHHT-TS-EEEE-TTTHHHHHHHHHHHHHT-HHHHHHHHHHHH---TTS--HHHHHHHHHH-HHHHHHHHHH-TT-ETEEEEEEEEETTTT----EEE---HHHHHHHHHHHHTT-HHHHHHHHHHHHHHHHHHHHHHHHTTTTETT-EEEEEEHHEEEEEEEEEEEEEEEE--TTEEEEEE----TT---TTEEE-HHHHHHHHHHHHH-\n",
            "-----EEE--S-HHHHHHHHT--TT-GGGHHHHHHHHHHHHHHHHHHHH----HHHHHHHHHHHHTT--GGHHHHHHHHHT-T-SSTHHHHHHHHHHHHHHHHHHT---TTT-EEEEEEEEEEETT-EEEEEEEETTTS--E----HHHHHHEE-SSS---GHHTT-SEEEEETTTSSHHHHHHHHHHHHHHT-EEEEEE---HHHHHHHHT-EEEEEE--GGG-EEEEEEEE-TTT-EEEEEETS-EEEEEEEEEEGGEEEE-GG-EE-HHH-ETTEEEEEEEEEETTTS-EEEE---EEEEEEE-TT--TTHHHHHHHHHH-TT--EEEEEEETTTTEEEEEEEEETT---EEEE-T-TE-EHEEEEEE-T---HHHHHHHH-HHHHHHHHHHHHHHHHHS--T-EE--S-HHHHHHHHT--TT-----HHHHHHHHHHH-HHHHHHH---HHHHHHHHHHHHHTT--GGHHHHHHHHHTTT-STTHHHHHHHHHHHHHHHHHHT---TTT--EEEEEEEEEETT-EEEEEEEEETTS--EEE--HHHEEEEEEEE----GGHETTEEEEEETT----HHHHHHHHHHHHHT--EEEE-TT-EHHHHHHTT-EEEEEEE-----EEEEEEEE-TTT-EEEEEETS--EEEEEEEE-GEEEEE----EE-HHG-ETTEEEEEEEEEETTTS-EEEEEEE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UAsGXi6t7urm"
      },
      "cell_type": "markdown",
      "source": [
        "# Accuracy Checking on the entire data set: after 47 epochs"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "g2wv3-NzawiA",
        "outputId": "ef460bc1-73f7-441a-ba9d-a2185bb53496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "p0HOF5UKLrfR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2b64d90d-942c-412c-803e-34f07f4628e5"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(X_train, y_train, batch_size=512)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5072/5072 [==============================] - 54s 11ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3048409574227378, 0.8947848136116654, 0.753533956004242]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "EqA7NE9mLms9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b91bccbd-e862-45e5-b891-251b96dc0b39"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(X_val, y_val, batch_size=512)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "564/564 [==============================] - 11s 19ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.588876522179191, 0.8264437508075795, 0.5948492833908569]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qOfcP9Ea7wj2",
        "outputId": "45ebad48-c932-4968-9c5a-d1b0b4a0e201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(train_input_data, train_target_data, batch_size=512)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5636/5636 [==============================] - 65s 12ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3332646773418186, 0.8879458543423476, 0.737693671472873]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "d3uvdiqeHTLb"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate the cvs file"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lm_uKrNZD1jg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def give_results(x, y_, revsere_decoder_index):\n",
        "    # print(\"input     : \" + str(x))\n",
        "    # print(\"prediction: \" + str(onehot_to_seq(y_, revsere_decoder_index).upper()))\n",
        "    return str(onehot_to_seq(y_, revsere_decoder_index).upper())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7r2ZqaQ2PVuZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def kaggle_csv(model,test_input_data,test_df):\n",
        "  y_test_pred = model.predict(test_input_data[:])\n",
        "  df = pd.DataFrame(columns=['id','expected'])\n",
        "  df['id'] = test_df['id']\n",
        "  expecteds = []\n",
        "  for i in range(len(test_input_data)):\n",
        "    expecteds.append(give_results(test_input_seqs[i], y_test_pred[i], revsere_decoder_index))\n",
        "  df['expected'] = expecteds\n",
        "  df.to_csv('kaggle_results_model_end.csv', header=True, index=False)\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jcBO5V8dn4-x",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kaggle_csv(model,test_input_data,test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TnllHP41G-8F"
      },
      "cell_type": "markdown",
      "source": [
        "# Dowload the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "L5OZT6YcW1-4",
        "outputId": "2f029442-b6b2-4060-9032-7b6e56cc0966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create & upload a file.\n",
        "uploaded = drive.CreateFile({'title': 'kaggle_results_model_end.csv'})\n",
        "uploaded.SetContentFile('kaggle_results_model_end.csv')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1P7UnhQGtaoUOkjD7bprPlRH9CVDTZbwB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6s1fe2m7G2w5"
      },
      "cell_type": "markdown",
      "source": [
        "# Draw the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EuXfE3t_GyA7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqIYFDMKe8f4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}